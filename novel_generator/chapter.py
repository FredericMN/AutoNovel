# novel_generator/chapter.py
# -*- coding: utf-8 -*-
"""
ç« èŠ‚è‰ç¨¿ç”ŸæˆåŠè·å–å†å²ç« èŠ‚æ–‡æœ¬ã€å½“å‰ç« èŠ‚æ‘˜è¦ç­‰
"""
import os
import json
import logging
import re  # æ·»åŠ reæ¨¡å—å¯¼å…¥
from core.adapters.llm_adapters import create_llm_adapter
from core.prompting.prompt_definitions import (
    first_chapter_draft_prompt,  # ç”¨äº fallback
    next_chapter_draft_prompt,  # ç”¨äº fallback
    summarize_recent_chapters_prompt,  # ç”¨äº fallback
    knowledge_filter_prompt,  # ç”¨äº fallback
    knowledge_search_prompt,  # ç”¨äº fallback
    chapter_critique_prompt,  # ğŸ†• æ‰¹è¯„å®¶æç¤ºè¯
    chapter_refine_prompt,    # ğŸ†• ä½œå®¶é‡å†™æç¤ºè¯
    resolve_global_system_prompt
)
from core.prompting.prompt_manager import PromptManager  # æ–°å¢ï¼šæç¤ºè¯ç®¡ç†å™¨
from core.utils.chapter_directory_parser import get_chapter_info_from_blueprint
from novel_generator.common import invoke_with_cleaning
from core.utils.file_utils import read_file, clear_file_content, save_string_to_txt, save_data_to_json, get_log_file_path
from novel_generator.vectorstore_utils import load_vector_store
from core.utils.volume_utils import (
    get_volume_number,
    is_volume_last_chapter,
    calculate_volume_ranges  # ä¼˜åŒ–ï¼šç»Ÿä¸€å¯¼å…¥ï¼Œé¿å…åŠ¨æ€å¯¼å…¥
)

def extract_volume_architecture(volume_arch_text: str, target_volume_num: int) -> str:
    """
    ä» Volume_architecture.txt ä¸­æå–æŒ‡å®šå·çš„æ¶æ„ä¿¡æ¯

    Args:
        volume_arch_text: Volume_architecture.txt çš„å®Œæ•´å†…å®¹
        target_volume_num: ç›®æ ‡å·å·ï¼ˆ1-basedï¼‰

    Returns:
        str: è¯¥å·çš„æ¶æ„æ–‡æœ¬ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²

    æ”¯æŒæ ¼å¼ï¼š
        ### **ç¬¬ä¸€å·ï¼ˆç¬¬1-10ç« ï¼‰**
        ### **ç¬¬1å·ï¼ˆç¬¬1-10ç« ï¼‰**
        ## ç¬¬äºŒå·
        ### ç¬¬ä¸‰å·
    """
    import re

    # åˆ†å‰²æ–‡æœ¬ä¸ºå·å—ï¼ˆé€šè¿‡æ ‡é¢˜è¡Œåˆ†å‰²ï¼‰
    # ä¼˜åŒ–æ­£åˆ™ï¼šåŒ¹é…è¡Œé¦–çš„æ ‡é¢˜æ ¼å¼ï¼Œæ’é™¤å†…å®¹ä¸­çš„å¼•ç”¨
    # å¿…é¡»ä»¥ # æˆ– * å¼€å¤´ï¼Œç¡®ä¿æ˜¯ Markdown æ ‡é¢˜
    # æ”¯æŒæ ¼å¼: ### **ç¬¬ä¸€å·ï¼ˆç¬¬1-10ç« ï¼‰** ç­‰
    volume_header_pattern = re.compile(
        r'^[#*]+\s*\**\s*ç¬¬\s*([é›¶ã€‡ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+)\s*å·',
        re.MULTILINE
    )

    matches = list(volume_header_pattern.finditer(volume_arch_text))
    if not matches:
        logging.warning("Volume_architecture.txt ä¸­æœªæ‰¾åˆ°å·æ ‡é¢˜")
        logging.debug(f"æ–‡ä»¶å†…å®¹å‰500å­—ç¬¦: {volume_arch_text[:500]}")
        return ""

    logging.info(f"æ‰¾åˆ°{len(matches)}ä¸ªå·æ ‡é¢˜æ ‡è®°")

    # è½¬æ¢ä¸­æ–‡æ•°å­—
    from core.utils.chapter_directory_parser import _to_int_from_chinese

    for i, match in enumerate(matches):
        vol_num_str = match.group(1)
        # å°è¯•æ•°å­—è½¬æ¢
        if vol_num_str.isdigit():
            vol_num = int(vol_num_str)
        else:
            vol_num = _to_int_from_chinese(vol_num_str)

        logging.debug(f"è§£æåˆ°å·å·: '{vol_num_str}' -> {vol_num}, ç›®æ ‡: {target_volume_num}")

        if vol_num == target_volume_num:
            # æ‰¾åˆ°ç›®æ ‡å·ï¼Œæå–ä»å½“å‰ä½ç½®åˆ°ä¸‹ä¸€ä¸ªå·æ ‡é¢˜ï¼ˆæˆ–æ–‡ä»¶æœ«å°¾ï¼‰çš„å†…å®¹
            start_pos = match.start()

            # æŸ¥æ‰¾åˆ†éš”ç¬¦ "---" æˆ–ä¸‹ä¸€ä¸ªå·æ ‡é¢˜
            if i + 1 < len(matches):
                end_pos = matches[i + 1].start()
            else:
                end_pos = len(volume_arch_text)

            # æå–å†…å®¹
            content = volume_arch_text[start_pos:end_pos].strip()

            # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„åˆ†éš”ç¬¦ "---"ï¼ˆæ”¯æŒå‰åéƒ½æœ‰çš„æƒ…å†µï¼‰
            content = re.sub(r'^[\s\n]*-{3,}[\s\n]*', '', content)  # ç§»é™¤å¼€å¤´
            content = re.sub(r'[\s\n]*-{3,}[\s\n]*$', '', content)  # ç§»é™¤ç»“å°¾
            content = content.strip()

            if content:
                logging.info(f"æˆåŠŸæå–ç¬¬{target_volume_num}å·æ¶æ„ï¼Œé•¿åº¦: {len(content)}å­—ç¬¦")
            else:
                logging.warning(f"ç¬¬{target_volume_num}å·æ¶æ„æå–åä¸ºç©º")
            return content

    logging.warning(f"Volume_architecture.txt ä¸­æœªæ‰¾åˆ°ç¬¬{target_volume_num}å·")
    return ""


logging.basicConfig(
    filename=get_log_file_path(),      # æ—¥å¿—æ–‡ä»¶å
    filemode='a',            # è¿½åŠ æ¨¡å¼ï¼ˆ'w' ä¼šè¦†ç›–ï¼‰
    level=logging.INFO,      # è®°å½• INFO åŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

def get_volume_context(
    filepath: str,
    novel_number: int,
    num_volumes: int,
    total_chapters: int  # æ–°å¢ï¼šå®é™…æ€»ç« èŠ‚æ•°
) -> dict:
    """
    è·å–å½“å‰ç« èŠ‚çš„åˆ†å·ä¸Šä¸‹æ–‡ä¿¡æ¯

    Args:
        filepath: å°è¯´ä¿å­˜è·¯å¾„
        novel_number: å½“å‰ç« èŠ‚å·
        num_volumes: æ€»å·æ•°ï¼ˆ0æˆ–1è¡¨ç¤ºä¸åˆ†å·ï¼‰
        total_chapters: å®é™…æ€»ç« èŠ‚æ•°

    Returns:
        dict: åˆ†å·ä¸Šä¸‹æ–‡ä¿¡æ¯
        {
            "is_volume_mode": bool,           # æ˜¯å¦åˆ†å·æ¨¡å¼
            "volume_number": int,             # å½“å‰å·å·ï¼ˆ1-basedï¼‰
            "is_volume_first_chapter": bool,  # æ˜¯å¦å·çš„ç¬¬ä¸€ç« 
            "is_volume_last_chapter": bool,   # æ˜¯å¦å·çš„æœ€åä¸€ç« 
            "volume_summary": str,            # å‰ä¸€å·çš„æ‘˜è¦ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            "current_volume_summary": str     # å½“å‰å·çš„æ‘˜è¦ï¼ˆå¦‚æœå·²å®Œæˆéƒ¨åˆ†ç« èŠ‚ï¼‰
        }
    """
    # éåˆ†å·æ¨¡å¼
    if num_volumes <= 1:
        return {
            "is_volume_mode": False,
            "volume_number": 0,
            "is_volume_first_chapter": False,
            "is_volume_last_chapter": False,
            "volume_summary": "",
            "current_volume_summary": ""
        }

    # åˆ†å·æ¨¡å¼
    volume_ranges = calculate_volume_ranges(total_chapters, num_volumes)
    volume_num = get_volume_number(novel_number, volume_ranges)

    if volume_num == 0:
        logging.warning(f"Chapter {novel_number} not found in volume ranges.")
        return {
            "is_volume_mode": True,
            "volume_number": 0,
            "is_volume_first_chapter": False,
            "is_volume_last_chapter": False,
            "volume_summary": "",
            "current_volume_summary": ""
        }

    vol_start, vol_end = volume_ranges[volume_num - 1]
    is_first = (novel_number == vol_start)
    is_last = is_volume_last_chapter(novel_number, volume_ranges)

    # è¯»å–å‰ä¸€å·æ‘˜è¦
    prev_volume_summary = ""
    if volume_num > 1:
        prev_vol_summary_file = os.path.join(filepath, f"volume_{volume_num - 1}_summary.txt")
        if os.path.exists(prev_vol_summary_file):
            prev_volume_summary = read_file(prev_vol_summary_file).strip()
        else:
            # é™çº§ç­–ç•¥ï¼šå¦‚æœå‰ä¸€å·æ‘˜è¦ä¸å­˜åœ¨ï¼Œä½¿ç”¨å…¨å±€æ‘˜è¦
            logging.warning(f"å‰ä¸€å·æ‘˜è¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨å…¨å±€æ‘˜è¦é™çº§")
            global_summary_file = os.path.join(filepath, "global_summary.txt")
            if os.path.exists(global_summary_file):
                prev_volume_summary = read_file(global_summary_file).strip()
                logging.info("å·²ä½¿ç”¨ global_summary.txt ä½œä¸ºå‰ä¸€å·æ‘˜è¦çš„é™çº§æ›¿ä»£")

    # è¯»å–å½“å‰å·æ‘˜è¦ï¼ˆå¦‚æœå·å·²ç»å®Œæˆå¹¶ç”Ÿæˆäº†æ‘˜è¦ï¼‰
    current_vol_summary = ""
    current_vol_summary_file = os.path.join(filepath, f"volume_{volume_num}_summary.txt")
    if os.path.exists(current_vol_summary_file):
        current_vol_summary = read_file(current_vol_summary_file).strip()

    return {
        "is_volume_mode": True,
        "volume_number": volume_num,
        "is_volume_first_chapter": is_first,
        "is_volume_last_chapter": is_last,
        "volume_summary": prev_volume_summary,
        "current_volume_summary": current_vol_summary
    }

def extract_key_plot_arcs(filepath: str) -> str:
    """
    [Plan A] ä» plot_arcs.txt ä¸­æå– Açº§(ä¸»çº¿)å’Œ Bçº§(æ”¯çº¿)çš„æœªè§£å†³ä¼ç¬”

    æ ¼å¼å…¼å®¹æ€§ï¼š
    - æ”¯æŒä»»æ„å‰å¯¼ç¬¦å·ï¼š-, â€¢, Â·, *, æ•°å­—ç¼–å·ç­‰
    - æ”¯æŒä»»æ„ç¼©è¿›
    - æ”¯æŒ [Açº§-xxx] æˆ– ã€Açº§-xxxã€‘ æ ¼å¼
    - è‡ªåŠ¨æ’é™¤å·²è§£å†³çš„ä¼ç¬”ï¼ˆé€šè¿‡æ£€æŸ¥åŒè¡Œçš„è§£å†³æ ‡è®°ï¼‰
    """
    plot_arcs_file = os.path.join(filepath, "plot_arcs.txt")
    if not os.path.exists(plot_arcs_file):
        return "ï¼ˆæš‚æ— è®°å½•ï¼‰"

    full_content = read_file(plot_arcs_file)
    if not full_content.strip():
        return "ï¼ˆæš‚æ— è®°å½•ï¼‰"

    # æå–æœªè§£å†³éƒ¨åˆ†
    unresolved_lines = []

    # æ›´å®½æ¾çš„æ­£åˆ™åŒ¹é…ï¼šä»»æ„ä½ç½®åŒ¹é… [Açº§-xxx] æˆ– ã€Açº§-xxxã€‘
    # æ ¼å¼ç¤ºä¾‹: [Açº§-ä¸»çº¿] xxxxã€ã€Bçº§-æ”¯çº¿ã€‘xxxã€1. [Açº§-ä¸»çº¿] xxx
    pattern = r'[\[ã€]([AB]çº§-[^\]ã€‘]+)[\]ã€‘]'

    # å·²è§£å†³æ ‡è®°ï¼šæ”¯æŒå¤šç§ç¬¦å·å’Œæ ¼å¼
    resolved_markers = ['âœ“å·²è§£å†³', 'âœ…å·²è§£å†³', 'â˜‘å·²è§£å†³', 'å·²è§£å†³', 'ï¼ˆå·²è§£å†³ï¼‰', '(å·²è§£å†³)', '[å·²è§£å†³]']

    for line in full_content.split('\n'):
        line_stripped = line.strip()
        if not line_stripped:
            continue

        # æ£€æŸ¥æœ¬è¡Œæ˜¯å¦åŒ…å«å·²è§£å†³æ ‡è®°
        is_resolved = any(marker in line_stripped for marker in resolved_markers)
        if is_resolved:
            continue

        # ä½¿ç”¨ search è€Œé matchï¼ŒåŒ¹é…è¡Œå†…ä»»æ„ä½ç½®çš„ A/B çº§æ ‡è®°
        if re.search(pattern, line_stripped):
            unresolved_lines.append(line_stripped)

    if not unresolved_lines:
        return "ï¼ˆæš‚æ— é‡è¦æœªè§£å†³ä¼ç¬”ï¼‰"

    # é™åˆ¶è¿”å›æ¡æ•°ï¼Œé¿å…è¿‡é•¿ï¼ˆAçº§æœ€å¤š10æ¡ï¼ŒBçº§æœ€å¤š5æ¡ï¼‰
    a_level = [l for l in unresolved_lines if re.search(r'[\[ã€]Açº§', l)]
    b_level = [l for l in unresolved_lines if re.search(r'[\[ã€]Bçº§', l)]

    result_lines = a_level[:10] + b_level[:5]

    if not result_lines:
        return "ï¼ˆæš‚æ— é‡è¦æœªè§£å†³ä¼ç¬”ï¼‰"

    logging.debug(f"Extracted {len(a_level)} A-level and {len(b_level)} B-level plot arcs")
    return "\n".join(result_lines)

def get_last_n_chapters_text(chapters_dir: str, current_chapter_num: int, n: int = 3) -> list:
    """
    [Plan B] è·å–æœ€è¿‘nç« å†…å®¹ï¼Œé‡‡ç”¨æ··åˆæ¨¡å¼ï¼š
    - N-1ç« ï¼šä¼˜å…ˆè¯»å–åŸæ–‡ (ä¿æŒæ–‡é£è¿è´¯)
    - N-2, N-3...ç« ï¼šä¼˜å…ˆè¯»å–æ‘˜è¦ (chapter_X_summary.txt)ï¼Œå‡å°‘tokenæ¶ˆè€—
    """
    texts = []
    start_chap = max(1, current_chapter_num - n)

    for c in range(start_chap, current_chapter_num):
        is_prev_chapter = (c == current_chapter_num - 1)

        # å°è¯•è¯»å–æ‘˜è¦ï¼ˆéä¸Šä¸€ç« æ—¶ä¼˜å…ˆï¼‰
        summary_file = os.path.join(chapters_dir, f"chapter_{c}_summary.txt")
        chap_file = os.path.join(chapters_dir, f"chapter_{c}.txt")

        text_content = ""
        source_type = ""

        # ç­–ç•¥ï¼š
        # 1. å¦‚æœæ˜¯ä¸Šä¸€ç«  (N-1)ï¼Œå¼ºåˆ¶è¯»åŸæ–‡ï¼ˆé™¤éåŸæ–‡ä¸å­˜åœ¨ï¼‰
        if is_prev_chapter:
            if os.path.exists(chap_file):
                text_content = read_file(chap_file).strip()
                source_type = "RAW"
            elif os.path.exists(summary_file):
                text_content = read_file(summary_file).strip()
                source_type = "SUMMARY_FALLBACK"
        # 2. å¦‚æœæ˜¯æ›´æ—©çš„ç« èŠ‚ (N-2, N-3...)ï¼Œä¼˜å…ˆè¯»æ‘˜è¦
        else:
            if os.path.exists(summary_file):
                text_content = read_file(summary_file).strip()
                source_type = "SUMMARY"
            elif os.path.exists(chap_file):
                text_content = read_file(chap_file).strip()
                source_type = "RAW_FALLBACK"

        if text_content:
            # åŒºåˆ†ä¸åŒæ¥æºçš„æ ‡è®°æ–¹å¼
            if source_type == "SUMMARY":
                texts.append(f"ã€ç¬¬{c}ç« æ‘˜è¦ã€‘\n{text_content}")
                logging.debug(f"Loaded summary for chapter {c}")
            elif source_type == "SUMMARY_FALLBACK":
                # ä¸Šä¸€ç« çš„åŸæ–‡ä¸å­˜åœ¨ï¼Œé™çº§ä½¿ç”¨æ‘˜è¦ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
                texts.append(f"ã€ç¬¬{c}ç« ï¼ˆä»…æ‘˜è¦å¯ç”¨ï¼‰ã€‘\n{text_content}")
                logging.debug(f"Loaded summary (fallback) for chapter {c}")
            else:
                # RAW æˆ– RAW_FALLBACKï¼šç›´æ¥ä½¿ç”¨åŸæ–‡
                texts.append(text_content)
                logging.debug(f"Loaded raw text for chapter {c}")
        else:
            texts.append("")

    return texts

def summarize_recent_chapters(
    interface_format: str,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int,
    chapters_text_list: list,
    novel_number: int,            # æ–°å¢å‚æ•°
    chapter_info: dict,           # æ–°å¢å‚æ•°
    next_chapter_info: dict,      # æ–°å¢å‚æ•°
    timeout: int = 600,
    system_prompt: str = ""
) -> str:  # ä¿®æ”¹è¿”å›å€¼ç±»å‹ä¸º strï¼Œä¸å†æ˜¯ tuple
    """
    æ ¹æ®å‰ä¸‰ç« å†…å®¹ç”Ÿæˆå½“å‰ç« èŠ‚çš„ç²¾å‡†æ‘˜è¦ã€‚
    å¢å¼ºå®¹é”™:ç©ºå€¼å…œåº•ã€æ ¼å¼åŒ–å¤±è´¥é‡è¯•ã€ä½¿ç”¨ç« èŠ‚ç›®å½•ä½œä¸ºåå¤‡ã€‚
    """
    try:
        combined_text = "\n".join(chapters_text_list).strip()
        if not combined_text:
            logging.warning("No previous chapters found, using chapter directory as fallback")
            # ç©ºå€¼å…œåº•:ä½¿ç”¨ç« èŠ‚ç›®å½•ä¿¡æ¯ç”Ÿæˆç®€è¦è¯´æ˜
            chapter_info = chapter_info or {}
            return f"å½“å‰ä¸ºç¬¬{novel_number}ç« ,å‰æ–‡å°šæ— å†…å®¹ã€‚æœ¬ç« å°†å›´ç»•ã€Œ{chapter_info.get('chapter_title', 'æœªå‘½å')}ã€å±•å¼€,æ ¸å¿ƒç›®æ ‡æ˜¯{chapter_info.get('chapter_purpose', 'æ¨è¿›å‰§æƒ…')}ã€‚"

        # é™åˆ¶ç»„åˆæ–‡æœ¬é•¿åº¦
        max_combined_length = 4000
        if len(combined_text) > max_combined_length:
            combined_text = combined_text[-max_combined_length:]

        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout
        )

        # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æœ‰é»˜è®¤å€¼
        chapter_info = chapter_info or {}
        next_chapter_info = next_chapter_info or {}

        # ä» PromptManager åŠ¨æ€åŠ è½½æç¤ºè¯ï¼ˆå¸¦å¼‚å¸¸ä¿æŠ¤ï¼‰
        try:
            pm = PromptManager()
        except Exception as e:
            logging.error(f"Failed to initialize PromptManager in summarize_recent_chapters: {e}")
            pm = None

        if pm:
            summary_prompt_template = pm.get_prompt("chapter", "chapter_summary")
        else:
            summary_prompt_template = None

        if not summary_prompt_template:
            logging.warning("Chapter summary prompt not found, using default")
            summary_prompt_template = summarize_recent_chapters_prompt

        prompt = summary_prompt_template.format(
            combined_text=combined_text,
            novel_number=novel_number,
            chapter_title=chapter_info.get("chapter_title", "æœªå‘½å"),
            chapter_role=chapter_info.get("chapter_role", "å¸¸è§„ç« èŠ‚"),
            chapter_purpose=chapter_info.get("chapter_purpose", "å†…å®¹æ¨è¿›"),
            suspense_level=chapter_info.get("suspense_level", "ä¸­ç­‰"),
            foreshadowing=chapter_info.get("foreshadowing", "æ— "),
            plot_twist_level=chapter_info.get("plot_twist_level", "â˜…â˜†â˜†â˜†â˜†"),
            chapter_summary=chapter_info.get("chapter_summary", ""),
            next_chapter_number=novel_number + 1,
            next_chapter_title=next_chapter_info.get("chapter_title", "ï¼ˆæœªå‘½åï¼‰"),
            next_chapter_role=next_chapter_info.get("chapter_role", "è¿‡æ¸¡ç« èŠ‚"),
            next_chapter_purpose=next_chapter_info.get("chapter_purpose", "æ‰¿ä¸Šå¯ä¸‹"),
            next_chapter_summary=next_chapter_info.get("chapter_summary", "è¡”æ¥è¿‡æ¸¡å†…å®¹"),
            next_chapter_suspense_level=next_chapter_info.get("suspense_level", "ä¸­ç­‰"),
            next_chapter_foreshadowing=next_chapter_info.get("foreshadowing", "æ— ç‰¹æ®Šä¼ç¬”"),
            next_chapter_plot_twist_level=next_chapter_info.get("plot_twist_level", "â˜…â˜†â˜†â˜†â˜†")
        )

        active_system_prompt = system_prompt.strip()

        # ç¬¬ä¸€æ¬¡å°è¯•ç”Ÿæˆæ‘˜è¦
        response_text = invoke_with_cleaning(
            llm_adapter,
            prompt,
            system_prompt=active_system_prompt
        )
        summary = extract_summary_from_response(response_text)

        if not summary or len(summary) < 50:
            logging.warning(f"First attempt summary too short ({len(summary) if summary else 0} chars), retrying with simplified prompt")

            # é‡è¯•:ä½¿ç”¨ç®€åŒ–çš„æç¤ºè¯
            simplified_prompt = f"""è¯·ä¸ºä»¥ä¸‹å‰æ–‡å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦(300-800å­—)ï¼š

å‰æ–‡å†…å®¹ï¼š
{combined_text}

å½“å‰è¦å†™çš„æ˜¯ç¬¬{novel_number}ç« ã€Š{chapter_info.get('chapter_title', 'æœªå‘½å')}ã€‹ã€‚

è¯·ç›´æ¥è¾“å‡ºæ‘˜è¦å†…å®¹,ä¸éœ€è¦ä»»ä½•å‰ç¼€æ ‡è®°ã€‚"""

            retry_response = invoke_with_cleaning(
                llm_adapter,
                simplified_prompt,
                system_prompt=active_system_prompt
            )

            # é‡è¯•å“åº”ä¹Ÿéœ€è¦ç»è¿‡æ ¼å¼æ¸…æ´—
            if retry_response:
                retry_summary = extract_summary_from_response(retry_response)
                # å¦‚æœæå–æˆåŠŸä¸”æ¯”ç¬¬ä¸€æ¬¡å¥½ï¼Œåˆ™ä½¿ç”¨é‡è¯•ç»“æœ
                if retry_summary and len(retry_summary) >= 50:
                    summary = retry_summary
                    logging.info(f"Retry successful, extracted {len(summary)} chars")
                elif retry_summary:
                    # æå–ç»“æœä»ç„¶å¤ªçŸ­ï¼Œä½†æ¯”åŸæ¥å¥½
                    summary = retry_summary
                    logging.warning(f"Retry summary still short ({len(retry_summary)} chars) but using it")
                # å¦‚æœæå–å®Œå…¨å¤±è´¥ï¼Œä¿æŒç¬¬ä¸€æ¬¡çš„ç»“æœä¸å˜
                else:
                    logging.warning("Retry extraction failed, keeping first attempt result")

        if not summary:
            logging.error("Failed to generate summary after retry, using fallback")
            # æœ€ç»ˆå…œåº•:ä½¿ç”¨ç« èŠ‚ç›®å½•ä¿¡æ¯
            fallback_summary = f"å‰æ–‡å·²å®Œæˆ{len(chapters_text_list)}ç« å†…å®¹ã€‚"
            if chapter_info.get("chapter_summary"):
                fallback_summary += f"æ¥ä¸‹æ¥ç¬¬{novel_number}ç« çš„æ ¸å¿ƒå†…å®¹æ˜¯:{chapter_info.get('chapter_summary')}"
            return fallback_summary

        return summary[:2000]  # é™åˆ¶æ‘˜è¦é•¿åº¦

    except Exception as e:
        logging.error(f"Error in summarize_recent_chapters: {str(e)}")
        # å¼‚å¸¸å…œåº•
        chapter_info = chapter_info or {}
        return f"[æ‘˜è¦ç”Ÿæˆå¼‚å¸¸] ç¬¬{novel_number}ç« ã€Š{chapter_info.get('chapter_title', 'æœªå‘½å')}ã€‹,å°†åŸºäºå‰æ–‡ç»§ç»­åˆ›ä½œã€‚"

def extract_summary_from_response(response_text: str) -> str:
    """
    ä»å“åº”æ–‡æœ¬ä¸­æå–æ‘˜è¦éƒ¨åˆ†,å¢å¼ºå®¹é”™èƒ½åŠ›

    æ”¯æŒå¤šç§æ ¼å¼:
    - æ ‡å‡†æ ¼å¼: "å½“å‰ç« èŠ‚æ‘˜è¦: xxx"
    - Markdownæ ¼å¼: "**æ‘˜è¦**: xxx" æˆ– "### æ‘˜è¦"
    - å¸¦è£…é¥°: "ã€æ‘˜è¦ã€‘xxx" æˆ– "â”â”æ‘˜è¦â”â”"
    """
    if not response_text:
        return ""

    # æ¸…ç†å¸¸è§çš„markdownæ ‡è®°
    cleaned = response_text.strip()

    # ç§»é™¤ä»£ç å—æ ‡è®°
    if cleaned.startswith("```"):
        parts = cleaned.split("```")
        if len(parts) >= 3:
            cleaned = parts[1]  # å–ä¸­é—´å†…å®¹

    # å®šä¹‰å¤šç§æ‘˜è¦æ ‡è®°,æŒ‰ä¼˜å…ˆçº§æ’åº
    summary_markers = [
        # æ ‡å‡†ä¸­æ–‡æ ‡è®°
        ("å½“å‰ç« èŠ‚æ‘˜è¦:", "å½“å‰ç« èŠ‚æ‘˜è¦ï¼š"),
        ("ç« èŠ‚æ‘˜è¦:", "ç« èŠ‚æ‘˜è¦ï¼š"),
        ("æœ¬ç« æ‘˜è¦:", "æœ¬ç« æ‘˜è¦ï¼š"),
        ("æ‘˜è¦:", "æ‘˜è¦ï¼š"),

        # Markdownæ ¼å¼
        ("**å½“å‰ç« èŠ‚æ‘˜è¦**:", "**å½“å‰ç« èŠ‚æ‘˜è¦**ï¼š"),
        ("**ç« èŠ‚æ‘˜è¦**:", "**ç« èŠ‚æ‘˜è¦**ï¼š"),
        ("**æ‘˜è¦**:", "**æ‘˜è¦**ï¼š"),
        ("### å½“å‰ç« èŠ‚æ‘˜è¦", "### ç« èŠ‚æ‘˜è¦", "### æ‘˜è¦"),
        ("## å½“å‰ç« èŠ‚æ‘˜è¦", "## ç« èŠ‚æ‘˜è¦", "## æ‘˜è¦"),

        # å¸¦è£…é¥°ç¬¦å·
        ("ã€å½“å‰ç« èŠ‚æ‘˜è¦ã€‘", "ã€ç« èŠ‚æ‘˜è¦ã€‘", "ã€æ‘˜è¦ã€‘"),
        ("â”â”å½“å‰ç« èŠ‚æ‘˜è¦â”â”", "â”â”ç« èŠ‚æ‘˜è¦â”â”", "â”â”æ‘˜è¦â”â”"),
        ("ã€Œå½“å‰ç« èŠ‚æ‘˜è¦ã€", "ã€Œç« èŠ‚æ‘˜è¦ã€", "ã€Œæ‘˜è¦ã€"),
    ]

    # å°è¯•åŒ¹é…æ‰€æœ‰æ ‡è®°
    for markers in summary_markers:
        if isinstance(markers, str):
            markers = (markers,)

        for marker in markers:
            if marker in cleaned:
                parts = cleaned.split(marker, 1)
                if len(parts) > 1:
                    extracted = parts[1].strip()

                    # ç§»é™¤å¯èƒ½çš„å°¾éƒ¨æ ‡è®°
                    for end_marker in ["```", "---", "***", "â”â”â”"]:
                        if end_marker in extracted:
                            extracted = extracted.split(end_marker)[0].strip()

                    # ç§»é™¤å¼€å¤´çš„å†’å·æˆ–ç©ºç™½
                    extracted = extracted.lstrip("ï¼š: \n\t")

                    if extracted:
                        logging.info(f"Successfully extracted summary using marker: {marker}")
                        return extracted

    # å¦‚æœæ‰€æœ‰æ ‡è®°éƒ½å¤±è´¥,å°è¯•å¯å‘å¼æå–
    # 1. æŸ¥æ‰¾ç¬¬ä¸€ä¸ª"ã€‚"ä¹‹åçš„é•¿æ–‡æœ¬å—(å¯èƒ½æ˜¯æ‘˜è¦)
    lines = cleaned.split('\n')
    potential_summary = []
    found_content = False

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # è·³è¿‡æ˜æ˜¾çš„æ ‡é¢˜è¡Œ
        if line.startswith('#') or line.startswith('**'):
            continue

        # è·³è¿‡è¿‡çŸ­çš„è¡Œ(å¯èƒ½æ˜¯æ ‡é¢˜)
        if len(line) < 20:
            continue

        # æ‰¾åˆ°å®è´¨å†…å®¹
        if len(line) >= 50 and 'ã€‚' in line:
            found_content = True
            potential_summary.append(line)
        elif found_content:
            # ç»§ç»­æ”¶é›†ç›¸é‚»è¡Œ
            potential_summary.append(line)
            if len('\n'.join(potential_summary)) > 500:
                break

    if potential_summary:
        extracted = '\n'.join(potential_summary)
        logging.warning(f"Used heuristic extraction, found {len(extracted)} chars")
        return extracted

    # æœ€åå…œåº•:è¿”å›åŸæ–‡(ä¼šåœ¨è°ƒç”¨å¤„æˆªæ–­)
    logging.warning("No marker matched, returning original response")
    return cleaned

def format_chapter_info(chapter_info: dict) -> str:
    """å°†ç« èŠ‚ä¿¡æ¯å­—å…¸æ ¼å¼åŒ–ä¸ºæ–‡æœ¬"""
    template = """
ç« èŠ‚ç¼–å·ï¼šç¬¬{number}ç« 
ç« èŠ‚æ ‡é¢˜ï¼šã€Š{title}ã€‹
ç« èŠ‚å®šä½ï¼š{role}
æ ¸å¿ƒä½œç”¨ï¼š{purpose}
ä¸»è¦äººç‰©ï¼š{characters}
å…³é”®é“å…·ï¼š{items}
åœºæ™¯åœ°ç‚¹ï¼š{location}
ä¼ç¬”è®¾è®¡ï¼š{foreshadow}
æ‚¬å¿µå¯†åº¦ï¼š{suspense}
è½¬æŠ˜ç¨‹åº¦ï¼š{twist}
ç« èŠ‚ç®€è¿°ï¼š{summary}
"""
    return template.format(
        number=chapter_info.get('chapter_number', 'æœªçŸ¥'),
        title=chapter_info.get('chapter_title', 'æœªçŸ¥'),
        role=chapter_info.get('chapter_role', 'æœªçŸ¥'),
        purpose=chapter_info.get('chapter_purpose', 'æœªçŸ¥'),
        characters=chapter_info.get('characters_involved', 'æœªæŒ‡å®š'),
        items=chapter_info.get('key_items', 'æœªæŒ‡å®š'),
        location=chapter_info.get('scene_location', 'æœªæŒ‡å®š'),
        foreshadow=chapter_info.get('foreshadowing', 'æ— '),
        suspense=chapter_info.get('suspense_level', 'ä¸€èˆ¬'),
        twist=chapter_info.get('plot_twist_level', 'â˜…â˜†â˜†â˜†â˜†'),
        summary=chapter_info.get('chapter_summary', 'æœªæä¾›')
    )

def parse_search_keywords(response_text: str) -> list:
    """
    è§£ææ£€ç´¢å…³é”®è¯ï¼Œæ”¯æŒå¤šç§æ ¼å¼å¹¶æä¾›å…œåº•ç­–ç•¥

    æ ‡å‡†æ ¼å¼ï¼š'ç§‘æŠ€å…¬å¸Â·æ•°æ®æ³„éœ²\nåœ°ä¸‹å®éªŒå®¤Â·åŸºå› ç¼–è¾‘'
    å…œåº•æ”¯æŒï¼š
    - ç©ºæ ¼åˆ†éš”ï¼š"ç§‘æŠ€å…¬å¸ æ•°æ®æ³„éœ²"
    - é¡¿å·åˆ†éš”ï¼š"ç§‘æŠ€å…¬å¸ã€æ•°æ®æ³„éœ²"
    - è¿å­—ç¬¦ï¼š"ç§‘æŠ€å…¬å¸-æ•°æ®æ³„éœ²"
    - çº¯æ–‡æœ¬è¡Œï¼ˆä½œä¸ºå•ä¸ªå…³é”®è¯ç»„ï¼‰

    Returns:
        list: å…³é”®è¯ç»„åˆ—è¡¨ï¼Œæœ€å¤š5ç»„ï¼Œç©ºå“åº”è¿”å›ç©ºåˆ—è¡¨
    """
    if not response_text or not response_text.strip():
        logging.warning("parse_search_keywords: Empty response, returning empty list")
        return []

    response_text = response_text.strip()

    # ç­–ç•¥1: æ ‡å‡†æ ¼å¼ - åŒ…å«ä¸­æ–‡é—´éš”å·Â·çš„è¡Œ
    keywords = [
        line.strip().replace('Â·', ' ')
        for line in response_text.split('\n')
        if 'Â·' in line and line.strip()
    ][:5]

    if keywords:
        logging.info(f"parse_search_keywords: Extracted {len(keywords)} keywords using standard format (Â·)")
        return keywords

    # ç­–ç•¥2: å…œåº•æ ¼å¼1 - åŒ…å«å…¶ä»–åˆ†éš”ç¬¦ï¼ˆã€- : |ï¼‰
    fallback_separators = ['ã€', '-', ':', '|']
    for sep in fallback_separators:
        keywords = [
            line.strip().replace(sep, ' ')
            for line in response_text.split('\n')
            if sep in line and line.strip()
        ][:5]

        if keywords:
            logging.warning(f"parse_search_keywords: Using fallback separator '{sep}', extracted {len(keywords)} keywords")
            return keywords

    # ç­–ç•¥3: å…œåº•æ ¼å¼2 - æŒ‰è¡Œåˆ†å‰²ï¼ˆæ¯è¡Œä½œä¸ºä¸€ä¸ªå…³é”®è¯ç»„ï¼‰
    lines = [line.strip() for line in response_text.split('\n') if line.strip()]

    # è¿‡æ»¤æ‰æ˜æ˜¾ä¸æ˜¯å…³é”®è¯çš„è¡Œï¼ˆå¦‚æ ‡é¢˜ã€è¯´æ˜æ–‡å­—ï¼‰
    # æ”¾å®½é•¿åº¦é™åˆ¶ï¼Œå…è®¸2å­—ç¬¦çŸ­å…³é”®è¯ï¼ˆå¦‚"AI""VR""ç§¦æœ"ï¼‰
    valid_lines = []
    filtered_lines = []  # è®°å½•è¢«è¿‡æ»¤çš„è¡Œ

    for line in lines:
        # å…è®¸é•¿åº¦2-50çš„è¡Œ
        if not (2 <= len(line) <= 50):
            filtered_lines.append((line, "length"))
            continue

        # æ’é™¤æ˜æ˜¾çš„æ ‡é¢˜å’Œè¯´æ˜æ–‡å­—
        if line.startswith(('æ³¨', 'è¯´æ˜', 'å¤‡æ³¨', '#', '*', 'æç¤º', 'å…³é”®è¯', 'æ£€ç´¢')):
            filtered_lines.append((line, "prefix"))
            continue

        if line.endswith(('ï¼š', ':')):
            filtered_lines.append((line, "suffix"))
            continue

        valid_lines.append(line)

    # è®°å½•è¢«è¿‡æ»¤çš„è¡Œåˆ°æ—¥å¿—ï¼ˆä»…è®°å½•å‰3ä¸ªï¼Œé¿å…æ—¥å¿—è¿‡é•¿ï¼‰
    if filtered_lines:
        sample = filtered_lines[:3]
        logging.debug(f"parse_search_keywords: Filtered {len(filtered_lines)} lines, sample: {sample}")

    if valid_lines:
        keywords = valid_lines[:5]
        logging.warning(f"parse_search_keywords: Using line-based fallback, extracted {len(keywords)} keywords")
        return keywords

    # ç­–ç•¥4: æœ€ç»ˆå…œåº• - ä½¿ç”¨æ•´ä¸ªå“åº”ä½œä¸ºå•ä¸ªå…³é”®è¯ï¼ˆæˆªæ–­åˆ°åˆç†é•¿åº¦ï¼‰
    if len(response_text) <= 100:
        logging.error(f"parse_search_keywords: All parsing strategies failed, using entire response as single keyword: '{response_text[:50]}...'")
        return [response_text]
    else:
        # å“åº”è¿‡é•¿ï¼Œå¯èƒ½æ˜¯LLMè¾“å‡ºäº†æ®µè½è€Œéå…³é”®è¯ï¼Œè®°å½•é”™è¯¯å¹¶è¿”å›ç©º
        logging.error(f"parse_search_keywords: Response too long ({len(response_text)} chars) and no valid format detected. Response preview: '{response_text[:100]}...'")
        return []

def extract_chapter_numbers(text: str) -> list:
    """
    ä»æ–‡æœ¬ä¸­æå–ç« èŠ‚ç¼–å·

    æ”¯æŒæ ¼å¼ï¼š
    - ç¬¬Nç«  / ç¬¬ N ç« ï¼ˆå…è®¸ç©ºæ ¼ï¼‰
    - chapter_N / chapter N / Chapter Nï¼ˆå…è®¸ç©ºæ ¼å’Œä¸‹åˆ’çº¿ï¼Œä¸åŒºåˆ†å¤§å°å†™ï¼‰
    """
    # åŒ¹é…"ç¬¬Nç« "æ ¼å¼ï¼ˆå…è®¸ç©ºæ ¼ï¼‰
    if re.search(r'ç¬¬\s*\d+\s*ç« ', text):
        return list(map(int, re.findall(r'ç¬¬\s*(\d+)\s*ç« ', text)))

    # åŒ¹é…"chapter N"æ ¼å¼ï¼ˆå…è®¸ç©ºæ ¼/ä¸‹åˆ’çº¿ï¼Œä¸åŒºåˆ†å¤§å°å†™ï¼‰
    elif re.search(r'chapter[_\s]*\d+', text, re.IGNORECASE):
        return list(map(int, re.findall(r'chapter[_\s]*(\d+)', text, re.IGNORECASE)))

    # å…œåº•:å°è¯•æå–æ‰€æœ‰æ•°å­—ï¼ˆä½†è¦è°¨æ…ï¼Œå¯èƒ½è¯¯åŒ¹é…ï¼‰
    nums = [int(s) for s in re.findall(r'\d+', text) if s.isdigit()]
    if nums:
        logging.debug(f"extract_chapter_numbers fallback: extracted {nums} from text: {text[:50]}...")
    return nums

def apply_unified_content_rules(texts: list, current_chapter: int) -> list:
    """
    ç»Ÿä¸€çš„å†…å®¹åˆ†ç±»ä¸è§„åˆ™å¤„ç†å‡½æ•°,åˆå¹¶åŸ apply_content_rules å’Œ apply_knowledge_rules é€»è¾‘ã€‚

    Args:
        texts: å¾…å¤„ç†çš„æ–‡æœ¬åˆ—è¡¨
        current_chapter: å½“å‰ç« èŠ‚å·

    Returns:
        å¤„ç†åçš„æ–‡æœ¬åˆ—è¡¨,å¸¦æœ‰è§„åˆ™æ ‡è®°
    """
    processed = []

    for text in texts:
        # æ£€æµ‹æ˜¯å¦åŒ…å«å†å²ç« èŠ‚æ ‡è®°ï¼ˆæ›´ä¸¥æ ¼çš„æ­£åˆ™ï¼‰
        has_chapter_marker = (
            re.search(r'ç¬¬\s*\d+\s*ç« ', text) or  # "ç¬¬Nç« "æ ¼å¼ï¼Œå…è®¸ç©ºæ ¼
            re.search(r'chapter[_\s]*\d+', text, re.IGNORECASE)  # "chapter_N"æˆ–"chapter N"ï¼Œä¸åŒºåˆ†å¤§å°å†™
        )

        if has_chapter_marker:
            # æå–ç« èŠ‚ç¼–å·
            chap_nums = extract_chapter_numbers(text)

            if chap_nums:
                recent_chap = max(chap_nums)
                time_distance = current_chapter - recent_chap

                # æ ¹æ®æ—¶é—´è·ç¦»åº”ç”¨ä¸åŒè§„åˆ™
                if time_distance <= 2:
                    # è¿‘2ç« :ç›´æ¥è·³è¿‡,é˜²æ­¢é‡å¤
                    processed.append(f"[SKIP] è·³è¿‡è¿‘ç« å†…å®¹({time_distance}ç« è·ç¦»): {text[:120]}...")
                    logging.info(f"Skipped recent chapter content (distance={time_distance}): {text[:50]}...")

                elif time_distance <= 3:
                    # ç¬¬3ç« :éœ€è¦é«˜åº¦ä¿®æ”¹
                    processed.append(f"[HISTORY_LIMIT] è¿‘æœŸç« èŠ‚é™åˆ¶(éœ€ä¿®æ”¹â‰¥50%): {text[:100]}...")
                    logging.debug(f"Marked as HISTORY_LIMIT (distance={time_distance})")

                elif time_distance <= 5:
                    # 3-5ç« å‰:å…è®¸å¼•ç”¨ä½†éœ€è¦ä¿®æ”¹
                    processed.append(f"[HISTORY_REF] å†å²å‚è€ƒ(éœ€æ”¹å†™â‰¥40%): {text}")
                    logging.debug(f"Marked as HISTORY_REF (distance={time_distance})")

                else:
                    # 6ç« ä»¥å‰:å¯ä»¥å¼•ç”¨æ ¸å¿ƒæ¦‚å¿µ
                    processed.append(f"[HISTORY_OK] è¿œæœŸç« èŠ‚(å¯å¼•ç”¨æ ¸å¿ƒ): {text}")
                    logging.debug(f"Marked as HISTORY_OK (distance={time_distance})")
            else:
                # æ— æ³•æå–ç« èŠ‚å·,ä½†æœ‰ç« èŠ‚æ ‡è®°,ä¿å®ˆå¤„ç†
                processed.append(f"[HISTORY_UNKNOWN] å†å²å†…å®¹(ç« èŠ‚å·ä¸æ˜): {text[:100]}...")
                logging.warning(f"Chapter marker found but no valid chapter number: {text[:50]}...")
        else:
            # éå†å²ç« èŠ‚å†…å®¹,åˆ¤æ–­ä¸ºå¤–éƒ¨çŸ¥è¯†,ä¼˜å…ˆä½¿ç”¨
            processed.append(f"[EXTERNAL] å¤–éƒ¨çŸ¥è¯†(ä¼˜å…ˆä½¿ç”¨): {text}")
            logging.debug(f"Marked as EXTERNAL knowledge: {text[:50]}...")

    return processed

def get_filtered_knowledge_context(
    api_key: str,
    base_url: str,
    model_name: str,
    interface_format: str,
    embedding_adapter,
    filepath: str,
    chapter_info: dict,
    retrieved_texts: list,
    max_tokens: int = 2048,
    timeout: int = 600,
    system_prompt: str = ""
) -> str:
    """
    ä¼˜åŒ–åçš„çŸ¥è¯†è¿‡æ»¤å¤„ç†

    æ³¨æ„ï¼šretrieved_texts åº”è¯¥æ˜¯å·²ç»è¿‡ apply_unified_content_rules å¤„ç†çš„æ–‡æœ¬åˆ—è¡¨
    """
    if not retrieved_texts:
        return "ï¼ˆæ— ç›¸å…³çŸ¥è¯†åº“å†…å®¹ï¼‰"

    try:
        # ç›´æ¥ä½¿ç”¨å·²å¤„ç†çš„æ–‡æœ¬ï¼Œä¸å†é‡å¤è°ƒç”¨è§„åˆ™å‡½æ•°
        processed_texts = retrieved_texts

        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=0.3,
            max_tokens=max_tokens,
            timeout=timeout
        )

        # é™åˆ¶æ£€ç´¢æ–‡æœ¬é•¿åº¦å¹¶æ ¼å¼åŒ–
        formatted_texts = []
        max_text_length = 600
        for i, text in enumerate(processed_texts, 1):
            if len(text) > max_text_length:
                text = text[:max_text_length] + "..."
            formatted_texts.append(f"[é¢„å¤„ç†ç»“æœ{i}]\n{text}")

        # ä½¿ç”¨æ ¼å¼åŒ–å‡½æ•°å¤„ç†ç« èŠ‚ä¿¡æ¯
        formatted_chapter_info = (
            f"å½“å‰ç« èŠ‚å®šä½ï¼š{chapter_info.get('chapter_role', '')}\n"
            f"æ ¸å¿ƒç›®æ ‡ï¼š{chapter_info.get('chapter_purpose', '')}\n"
            f"å…³é”®è¦ç´ ï¼š{chapter_info.get('characters_involved', '')} | "
            f"{chapter_info.get('key_items', '')} | "
            f"{chapter_info.get('scene_location', '')}"
        )

        # ä» PromptManager åŠ¨æ€åŠ è½½æç¤ºè¯ï¼ˆå¸¦å¼‚å¸¸ä¿æŠ¤ï¼‰
        try:
            pm = PromptManager()
        except Exception as e:
            logging.error(f"Failed to initialize PromptManager in filter_knowledge_context: {e}")
            pm = None

        if pm:
            filter_prompt_template = pm.get_prompt("helper", "knowledge_filter")
        else:
            filter_prompt_template = None

        if not filter_prompt_template:
            logging.warning("Knowledge filter prompt not found, using default")
            filter_prompt_template = knowledge_filter_prompt

        prompt = filter_prompt_template.format(
            chapter_info=formatted_chapter_info,
            retrieved_texts="\n\n".join(formatted_texts) if formatted_texts else "ï¼ˆæ— æ£€ç´¢ç»“æœï¼‰"
        )

        filtered_content = invoke_with_cleaning(llm_adapter, prompt, system_prompt=system_prompt)
        return filtered_content if filtered_content else "ï¼ˆçŸ¥è¯†å†…å®¹è¿‡æ»¤å¤±è´¥ï¼‰"

    except TimeoutError as e:
        # è¶…æ—¶å¼‚å¸¸
        import traceback
        logging.error(f"Knowledge filtering timeout after {timeout}s: {str(e)}\n{traceback.format_exc()}")
        return "ï¼ˆçŸ¥è¯†è¿‡æ»¤è¶…æ—¶ï¼Œå»ºè®®å¢åŠ timeoutå‚æ•°æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼‰"

    except Exception as e:
        # å…¶ä»–å¼‚å¸¸ï¼šAPIè®¤è¯ã€æ¨¡å‹é”™è¯¯ã€å‚æ•°é”™è¯¯ç­‰
        import traceback
        error_msg = str(e).lower()
        error_details = traceback.format_exc()

        # è®°å½•å®Œæ•´å †æ ˆåˆ°æ—¥å¿—
        logging.error(f"Error in knowledge filtering: {str(e)}\n{error_details}")

        # æ ¹æ®é”™è¯¯ç±»å‹è¿”å›ä¸åŒæç¤º
        if "api" in error_msg and ("key" in error_msg or "auth" in error_msg or "unauthorized" in error_msg):
            return "ï¼ˆAPIè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥api_keyé…ç½®æ˜¯å¦æ­£ç¡®ï¼‰"

        elif "connection" in error_msg or "network" in error_msg or "unreachable" in error_msg:
            return "ï¼ˆç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥base_urlå’Œç½‘ç»œçŠ¶æ€ï¼‰"

        elif "rate" in error_msg and "limit" in error_msg:
            return "ï¼ˆAPIè°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡çº§é…é¢ï¼‰"

        elif "model" in error_msg and ("not found" in error_msg or "invalid" in error_msg):
            return f"ï¼ˆæ¨¡å‹ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥model_nameé…ç½®: {model_name}ï¼‰"

        elif "token" in error_msg and ("limit" in error_msg or "exceed" in error_msg):
            return "ï¼ˆTokenæ•°é‡è¶…é™ï¼Œè¯·å‡å°‘max_tokensæˆ–ç®€åŒ–è¾“å…¥å†…å®¹ï¼‰"

        else:
            # æœªçŸ¥é”™è¯¯ï¼Œè¿”å›å‰100å­—ç¬¦çš„é”™è¯¯ä¿¡æ¯
            error_preview = str(e)[:100]
            return f"ï¼ˆå†…å®¹è¿‡æ»¤å‡ºé”™ï¼š{error_preview}{'...' if len(str(e)) > 100 else ''}ï¼‰"

def build_chapter_prompt(
    api_key: str,
    base_url: str,
    model_name: str,
    filepath: str,
    novel_number: int,
    word_number: int,
    temperature: float,
    user_guidance: str,
    characters_involved: str,
    key_items: str,
    scene_location: str,
    time_constraint: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    embedding_retrieval_k: int = 2,
    interface_format: str = "openai",
    max_tokens: int = 2048,
    timeout: int = 600,
    system_prompt: str = "",
    num_volumes: int = 0,  # æ–°å¢ï¼šåˆ†å·æ•°é‡
    total_chapters: int = 0,  # æ–°å¢ï¼šæ€»ç« èŠ‚æ•°
    gui_log_callback=None,
    progress_callback=None  # ğŸ†• è¿›åº¦å›è°ƒå‡½æ•°
) -> str:
    """
    æ„é€ å½“å‰ç« èŠ‚çš„è¯·æ±‚æç¤ºè¯ï¼ˆå®Œæ•´å®ç°ç‰ˆï¼‰
    ä¿®æ”¹é‡ç‚¹ï¼š
    1. ä¼˜åŒ–çŸ¥è¯†åº“æ£€ç´¢æµç¨‹
    2. æ–°å¢å†…å®¹é‡å¤æ£€æµ‹æœºåˆ¶
    3. é›†æˆæç¤ºè¯åº”ç”¨è§„åˆ™
    """
    # GUIæ—¥å¿—è¾…åŠ©å‡½æ•°
    def gui_log(msg):
        if gui_log_callback:
            gui_log_callback(msg)
        logging.info(msg)

    # è¿›åº¦æ›´æ–°è¾…åŠ©å‡½æ•°
    def update_progress(msg, pct):
        try:
            if progress_callback:
                progress_callback(msg, pct)
        except Exception as e:
            logging.warning(f"è¿›åº¦å›è°ƒå¤±è´¥: {e}")

    # é˜¶æ®µ1å¼€å§‹ï¼š0% - ğŸ“‚ å‡†å¤‡ä¸­
    update_progress("ğŸ“‚ å‡†å¤‡ä¸­...", 0.0)

    # è¯»å–åŸºç¡€æ–‡ä»¶ï¼š5%
    update_progress("ğŸ“– è¯»å–åŸºç¡€æ–‡ä»¶", 0.05)
    arch_file = os.path.join(filepath, "Novel_architecture.txt")
    novel_architecture_text = read_file(arch_file)
    directory_file = os.path.join(filepath, "Novel_directory.txt")
    blueprint_text = read_file(directory_file)
    global_summary_file = os.path.join(filepath, "global_summary.txt")
    global_summary_text = read_file(global_summary_file)
    character_state_file = os.path.join(filepath, "character_state.txt")
    character_state_text = read_file(character_state_file)

    # [Plan A] è¯»å–å…³é”®å‰§æƒ…ä¼ç¬”
    unresolved_plot_arcs = extract_key_plot_arcs(filepath)
    if unresolved_plot_arcs and unresolved_plot_arcs != "ï¼ˆæš‚æ— è®°å½•ï¼‰":
        logging.info(f"Injecting {len(unresolved_plot_arcs.splitlines())} unresolved plot arcs into prompt")
    else:
        unresolved_plot_arcs = "ï¼ˆæš‚æ— æ˜ç¡®è®°å½•ï¼‰"

    # è·å–ç« èŠ‚ä¿¡æ¯
    chapter_info = get_chapter_info_from_blueprint(blueprint_text, novel_number)
    chapter_title = chapter_info["chapter_title"]
    chapter_role = chapter_info["chapter_role"]
    chapter_purpose = chapter_info["chapter_purpose"]
    suspense_level = chapter_info["suspense_level"]
    foreshadowing = chapter_info["foreshadowing"]
    plot_twist_level = chapter_info["plot_twist_level"]
    chapter_summary = chapter_info["chapter_summary"]
    volume_position = chapter_info.get("volume_position", "")  # ğŸ†• æ–°å¢ï¼šæå–å·å†…ä½ç½®

    # æå–å·ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
    current_vol_num = chapter_info.get("volume_number")
    current_vol_title = chapter_info.get("volume_title", "")

    # è¯»å–å½“å‰å·æ¶æ„ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
    current_volume_architecture = ""

    # ä¼˜åŒ–åˆ¤æ–­é€»è¾‘ï¼šå³ä½¿ current_vol_num ä¸ºç©ºï¼Œä¹Ÿå°è¯•æ ¹æ® num_volumes æ¨æ–­
    if num_volumes > 1:
        volume_arch_file = os.path.join(filepath, "Volume_architecture.txt")
        if os.path.exists(volume_arch_file):
            volume_arch_text = read_file(volume_arch_file).strip()

            # å¦‚æœç« èŠ‚ä¿¡æ¯ä¸­æ²¡æœ‰å·å·ï¼Œå°è¯•æ ¹æ®ç« èŠ‚å·æ¨æ–­
            if not current_vol_num:
                volume_ranges = calculate_volume_ranges(total_chapters, num_volumes)
                current_vol_num = get_volume_number(novel_number, volume_ranges)
                logging.info(f"ç« èŠ‚ {novel_number} æ ¹æ®èŒƒå›´æ¨æ–­å·å·ä¸º: {current_vol_num}")

            # æå–å½“å‰å·çš„æ¶æ„ä¿¡æ¯
            if current_vol_num:
                current_volume_architecture = extract_volume_architecture(
                    volume_arch_text,
                    current_vol_num
                )
                if current_volume_architecture:
                    logging.info(f"æˆåŠŸä¸ºç¬¬{novel_number}ç« æå–ç¬¬{current_vol_num}å·æ¶æ„")
                else:
                    logging.warning(f"ç¬¬{novel_number}ç« æå–ç¬¬{current_vol_num}å·æ¶æ„å¤±è´¥")
            else:
                logging.warning(f"æ— æ³•ç¡®å®šç¬¬{novel_number}ç« çš„å·å·,è·³è¿‡å·æ¶æ„æå–")
        else:
            logging.warning(f"åˆ†å·æ¨¡å¼å·²å¯ç”¨ä½† Volume_architecture.txt ä¸å­˜åœ¨: {volume_arch_file}")

    # è·å–ä¸‹ä¸€ç« èŠ‚ä¿¡æ¯
    next_chapter_number = novel_number + 1
    next_chapter_info = get_chapter_info_from_blueprint(blueprint_text, next_chapter_number)
    next_chapter_title = next_chapter_info.get("chapter_title", "ï¼ˆæœªå‘½åï¼‰")
    next_chapter_role = next_chapter_info.get("chapter_role", "è¿‡æ¸¡ç« èŠ‚")
    next_chapter_purpose = next_chapter_info.get("chapter_purpose", "æ‰¿ä¸Šå¯ä¸‹")
    next_chapter_suspense = next_chapter_info.get("suspense_level", "ä¸­ç­‰")
    next_chapter_foreshadow = next_chapter_info.get("foreshadowing", "æ— ç‰¹æ®Šä¼ç¬”")
    next_chapter_twist = next_chapter_info.get("plot_twist_level", "â˜…â˜†â˜†â˜†â˜†")
    next_chapter_summary = next_chapter_info.get("chapter_summary", "è¡”æ¥è¿‡æ¸¡å†…å®¹")

    # æå–ä¸‹ä¸€ç« å·ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
    next_vol_num = next_chapter_info.get("volume_number")
    next_vol_title = next_chapter_info.get("volume_title", "")

    # æ„å»ºå·ä¿¡æ¯å±•ç¤ºå­—ç¬¦ä¸²ï¼ˆæ–°å¢ï¼‰
    if current_vol_num and current_vol_title:
        current_volume_display = f"ç¬¬{current_vol_num}å·ï¼š{current_vol_title}"
    else:
        current_volume_display = ""

    if next_vol_num and next_vol_title:
        next_volume_display = f"ç¬¬{next_vol_num}å·ï¼š{next_vol_title}"
    else:
        next_volume_display = ""

    # åˆ›å»ºç« èŠ‚ç›®å½•
    chapters_dir = os.path.join(filepath, "chapters")
    os.makedirs(chapters_dir, exist_ok=True)

    # ç¬¬ä¸€ç« ç‰¹æ®Šå¤„ç†
    if novel_number == 1:
        # ä» PromptManager åŠ¨æ€åŠ è½½æç¤ºè¯ï¼ˆå¸¦å¼‚å¸¸ä¿æŠ¤ï¼‰
        try:
            pm = PromptManager()
        except Exception as e:
            logging.error(f"Failed to initialize PromptManager in build_first_chapter_prompt: {e}")
            pm = None

        if pm:
            first_prompt_template = pm.get_prompt("chapter", "first_chapter")
        else:
            first_prompt_template = None

        if not first_prompt_template:
            logging.warning("First chapter prompt not found, using default")
            first_prompt_template = first_chapter_draft_prompt

        return first_prompt_template.format(
            volume_display=current_volume_display,  # æ–°å¢ï¼šä¼ é€’å·ä¿¡æ¯
            volume_architecture=current_volume_architecture,  # æ–°å¢ï¼šä¼ é€’å·æ¶æ„
            novel_number=novel_number,
            word_number=word_number,
            chapter_title=chapter_title,
            chapter_role=chapter_role,
            chapter_purpose=chapter_purpose,
            suspense_level=suspense_level,
            foreshadowing=foreshadowing,
            plot_twist_level=plot_twist_level,
            volume_position=volume_position,  # ğŸ†• æ–°å¢ï¼šä¼ é€’å·å†…ä½ç½®
            chapter_summary=chapter_summary,
            characters_involved=characters_involved,
            key_items=key_items,
            scene_location=scene_location,
            time_constraint=time_constraint,
            user_guidance=user_guidance,
            novel_setting=novel_architecture_text,
            unresolved_plot_arcs=unresolved_plot_arcs  # ğŸ†• æ³¨å…¥ä¼ç¬”
        )

    # è·å–å‰æ–‡å†…å®¹å’Œæ‘˜è¦
    recent_texts = get_last_n_chapters_text(chapters_dir, novel_number, n=3)

    # è·å–åˆ†å·ä¸Šä¸‹æ–‡
    volume_context = get_volume_context(filepath, novel_number, num_volumes, total_chapters)

    # æ„å»ºåˆ†å·ä¿¡æ¯å­—ç¬¦ä¸²
    volume_info_text = ""
    if volume_context["is_volume_mode"]:
        vol_num = volume_context["volume_number"]
        volume_info_parts = [f"å½“å‰å·å·ï¼šç¬¬{vol_num}å·"]

        if volume_context["is_volume_first_chapter"]:
            volume_info_parts.append("ç« èŠ‚å®šä½ï¼šæœ¬å·é¦–ç« ")
        elif volume_context["is_volume_last_chapter"]:
            volume_info_parts.append("ç« èŠ‚å®šä½ï¼šæœ¬å·æœ«ç« ï¼ˆéœ€ä¸ºæœ¬å·æ”¶å°¾ï¼‰")

        volume_info_text = "\n".join(volume_info_parts)

        # æ–°ç­–ç•¥ï¼šå›ºå®šä¼ é€’"ä¸Šä¸€å·å®Œæ•´æ‘˜è¦ + æœ¬å·ç´¯ç§¯æ‘˜è¦"
        # ä¼˜åŠ¿ï¼šä¿è¯è·¨å·è¿è´¯æ€§ï¼Œé¿å…è¿‡æ—©ä¸¢å¤±å‰ä¸€å·ä¿¡æ¯
        if volume_context["volume_summary"]:
            # æ„å»ºæ‘˜è¦ï¼šä¸Šä¸€å· + æœ¬å·
            combined_summary = f"ã€ä¸Šä¸€å·å®Œæ•´å›é¡¾ã€‘\n{volume_context['volume_summary']}\n\n"

            # æ·»åŠ æœ¬å·ç´¯ç§¯æ‘˜è¦
            if global_summary_text.strip():
                combined_summary += f"ã€æœ¬å·è¿›å±•ã€‘\n{global_summary_text}"
            else:
                combined_summary += "ã€æœ¬å·è¿›å±•ã€‘\nï¼ˆæœ¬å·å°šæœªå¼€å§‹ç´¯ç§¯æ‘˜è¦ï¼‰"

            global_summary_text = combined_summary
            logging.info(f"ç¬¬{novel_number}ç« ä½¿ç”¨å›ºå®šç­–ç•¥: ä¸Šä¸€å·å®Œæ•´æ‘˜è¦ + æœ¬å·ç´¯ç§¯æ‘˜è¦")
        else:
            # ç¬¬ä¸€å·æˆ–æ— å‰å·æ‘˜è¦ï¼šä»…ä½¿ç”¨æœ¬å·ç´¯ç§¯
            logging.info(f"ç¬¬{novel_number}ç« ä½¿ç”¨æœ¬å·ç´¯ç§¯æ‘˜è¦ï¼ˆæ— å‰å·ä¿¡æ¯ï¼‰")

        # å¦åˆ™ç»§ç»­ä½¿ç”¨å…¨å±€æ‘˜è¦ï¼ˆé»˜è®¤ï¼‰
    else:
        # éåˆ†å·æ¨¡å¼ï¼šä¸æ˜¾ç¤ºåˆ†å·ä¿¡æ¯
        volume_info_text = ""

    # ç”Ÿæˆå‰æ–‡æ‘˜è¦ï¼š10%
    update_progress("ğŸ“ ç”Ÿæˆå‰æ–‡æ‘˜è¦", 0.10)
    try:
        logging.info("Attempting to generate summary")
        short_summary = summarize_recent_chapters(
            interface_format=interface_format,
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            temperature=temperature,
            max_tokens=max_tokens,
            chapters_text_list=recent_texts,
            novel_number=novel_number,
            chapter_info=chapter_info,
            next_chapter_info=next_chapter_info,
            timeout=timeout,
            system_prompt=system_prompt
        )
        logging.info("Summary generated successfully")
    except Exception as e:
        logging.error(f"Error in summarize_recent_chapters: {str(e)}")
        short_summary = "ï¼ˆæ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼‰"

    # è·å–å‰ä¸€ç« ç»“å°¾
    previous_excerpt = ""
    for text in reversed(recent_texts):
        if text.strip():
            previous_excerpt = text[-800:] if len(text) > 800 else text
            break

    # ç¼“å­˜æœ¬ç« ä¸Šä¸‹æ–‡ï¼ˆä¾› Plan C é‡å†™ä½¿ç”¨ï¼Œé¿å…é‡å¤è®¡ç®—ï¼‰
    if novel_number > 1:
        context_cache = {
            "novel_number": novel_number,
            "short_summary": short_summary,
            "previous_chapter_excerpt": previous_excerpt
        }
        context_cache_file = os.path.join(chapters_dir, f"chapter_{novel_number}_context.json")
        try:
            save_data_to_json(context_cache, context_cache_file)
            logging.info(f"Saved Plan C context cache: {context_cache_file}")
        except Exception as e:
            logging.warning(f"Failed to save Plan C context cache: {e}")

    # çŸ¥è¯†åº“æ£€ç´¢å’Œå¤„ç†
    try:
        gui_log("\nâ”â”â”â” çŸ¥è¯†åº“æ£€ç´¢ â”â”â”â”")
        gui_log("â–¶ å¼€å§‹å‘é‡æ£€ç´¢æµç¨‹...")

        # ç”Ÿæˆæ£€ç´¢å…³é”®è¯ï¼š15%
        update_progress("ğŸ” ç”Ÿæˆæ£€ç´¢å…³é”®è¯", 0.15)
        gui_log("   â”œâ”€ ç”Ÿæˆæ£€ç´¢å…³é”®è¯...")
        llm_adapter = create_llm_adapter(
            interface_format=interface_format,
            base_url=base_url,
            model_name=model_name,
            api_key=api_key,
            temperature=0.3,
            max_tokens=max_tokens,
            timeout=timeout
        )

        # ä» PromptManager åŠ¨æ€åŠ è½½æç¤ºè¯ï¼ˆå¸¦å¼‚å¸¸ä¿æŠ¤ï¼‰
        try:
            pm = PromptManager()
        except Exception as e:
            logging.error(f"Failed to initialize PromptManager in build_next_chapter_prompt (knowledge search): {e}")
            pm = None

        if pm:
            search_prompt_template = pm.get_prompt("helper", "knowledge_search")
        else:
            search_prompt_template = None

        if not search_prompt_template:
            logging.warning("Knowledge search prompt not found, using default")
            search_prompt_template = knowledge_search_prompt

        search_prompt = search_prompt_template.format(
            chapter_number=novel_number,
            chapter_title=chapter_title,
            characters_involved=characters_involved,
            key_items=key_items,
            scene_location=scene_location,
            chapter_role=chapter_role,
            chapter_purpose=chapter_purpose,
            foreshadowing=foreshadowing,
            short_summary=short_summary,
            user_guidance=user_guidance,
            time_constraint=time_constraint
        )

        search_response = invoke_with_cleaning(llm_adapter, search_prompt, system_prompt=system_prompt)
        keyword_groups = parse_search_keywords(search_response)

        if keyword_groups:
            gui_log(f"   â”œâ”€ ç”Ÿæˆå…³é”®è¯ç»„: {len(keyword_groups)}ç»„")
            for idx, kw in enumerate(keyword_groups, 1):
                gui_log(f"       {idx}. {kw}")
        else:
            gui_log("   â”œâ”€ âš  æœªèƒ½ç”Ÿæˆå…³é”®è¯ï¼Œè·³è¿‡æ£€ç´¢")

        # æ‰§è¡Œå‘é‡æ£€ç´¢(ä½¿ç”¨å»é‡ä¼˜åŒ–çš„æ‰¹é‡æ£€ç´¢)ï¼š20%
        update_progress("ğŸ—‚ï¸ æ‰§è¡Œå‘é‡æ£€ç´¢", 0.20)
        from core.adapters.embedding_adapters import create_embedding_adapter
        from novel_generator.vectorstore_utils import get_relevant_contexts_deduplicated

        embedding_adapter = create_embedding_adapter(
            embedding_interface_format,
            embedding_api_key,
            embedding_url,
            embedding_model_name
        )

        gui_log("   â”œâ”€ æ‰§è¡Œå‘é‡æ£€ç´¢...")
        # ä½¿ç”¨æ–°çš„å»é‡æ£€ç´¢å‡½æ•°ï¼ˆæ”¯æŒåˆ†å·æ£€ç´¢ï¼‰
        retrieved_docs = get_relevant_contexts_deduplicated(
            embedding_adapter=embedding_adapter,
            query_groups=keyword_groups,
            filepath=filepath,
            k_per_group=embedding_retrieval_k,
            max_total_results=embedding_retrieval_k * len(keyword_groups) if keyword_groups else 10,
            current_chapter=novel_number,  # æ–°å¢ï¼šå½“å‰ç« èŠ‚å·
            num_volumes=num_volumes,  # æ–°å¢ï¼šæ€»å·æ•°
            total_chapters=total_chapters  # æ–°å¢ï¼šæ€»ç« èŠ‚æ•°
        )

        # è®°å½•æ£€ç´¢ç»Ÿè®¡
        from novel_generator.vectorstore_monitor import log_retrieval

        gui_log(f"   â”œâ”€ æ£€ç´¢ç»“æœ: å…±{len(retrieved_docs)}æ¡æ–‡æ¡£")

        # ç»Ÿè®¡æ–‡æ¡£ç±»å‹
        type_counts = {}
        for doc_info in retrieved_docs:
            doc_type = doc_info.get("type", "Unknown")
            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1

        if type_counts:
            gui_log("   â”œâ”€ æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
            for doc_type, count in type_counts.items():
                gui_log(f"       Â· {doc_type}: {count}æ¡")

        for keyword_group in keyword_groups:
            # ä¸ºæ¯ä¸ªå…³é”®è¯ç»„æ‰¾åˆ°æ‰€æœ‰å‘½ä¸­çš„æ–‡æ¡£
            docs_for_group = [
                {"content": d["content"], "type": d["type"]}
                for d in retrieved_docs
                if keyword_group in d.get("queries", [])
            ]
            log_retrieval(
                filepath=filepath,
                query=keyword_group,
                retrieved_docs=docs_for_group,
                chapter_number=novel_number
            )

        # æ ¼å¼åŒ–æ£€ç´¢ç»“æœ
        all_contexts = []
        for doc_info in retrieved_docs:
            content = doc_info["content"]
            doc_type = doc_info["type"]
            all_contexts.append(f"[{doc_type}] {content}")

        # åº”ç”¨ç»Ÿä¸€çš„å†…å®¹è§„åˆ™ï¼š25%
        update_progress("ğŸ”§ åº”ç”¨å†…å®¹è¿‡æ»¤è§„åˆ™", 0.25)
        gui_log("   â”œâ”€ åº”ç”¨å†…å®¹è¿‡æ»¤è§„åˆ™...")
        processed_contexts = apply_unified_content_rules(all_contexts, novel_number)

        # ç»Ÿè®¡è¿‡æ»¤ç»“æœ
        skip_count = sum(1 for ctx in processed_contexts if ctx.startswith("[SKIP]"))
        external_count = sum(1 for ctx in processed_contexts if ctx.startswith("[EXTERNAL]"))
        history_count = len(processed_contexts) - skip_count - external_count

        gui_log(f"   â”œâ”€ è¿‡æ»¤ç»Ÿè®¡:")
        gui_log(f"       Â· è·³è¿‡è¿‘ç« å†…å®¹: {skip_count}æ¡")
        gui_log(f"       Â· å¤–éƒ¨çŸ¥è¯†: {external_count}æ¡")
        gui_log(f"       Â· å†å²å‚è€ƒ: {history_count}æ¡")

        # æ‰§è¡ŒçŸ¥è¯†è¿‡æ»¤ï¼š30%
        update_progress("ğŸ§  LLMäºŒæ¬¡è¿‡æ»¤ä¸æ•´åˆ", 0.30)
        gui_log("   â”œâ”€ LLMäºŒæ¬¡è¿‡æ»¤ä¸æ•´åˆ...")
        chapter_info_for_filter = {
            "chapter_number": novel_number,
            "chapter_title": chapter_title,
            "chapter_role": chapter_role,
            "chapter_purpose": chapter_purpose,
            "characters_involved": characters_involved,
            "key_items": key_items,
            "scene_location": scene_location,
            "foreshadowing": foreshadowing,  # ä¿®å¤æ‹¼å†™é”™è¯¯
            "suspense_level": suspense_level,
            "plot_twist_level": plot_twist_level,
            "chapter_summary": chapter_summary,
            "time_constraint": time_constraint
        }
        
        filtered_context = get_filtered_knowledge_context(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            interface_format=interface_format,
            embedding_adapter=embedding_adapter,
            filepath=filepath,
            chapter_info=chapter_info_for_filter,
            retrieved_texts=processed_contexts,
            max_tokens=max_tokens,
            timeout=timeout,
            system_prompt=system_prompt
        )

        # ç»Ÿè®¡æœ€ç»ˆä½¿ç”¨çš„çŸ¥è¯†
        final_length = len(filtered_context)
        gui_log(f"   â””â”€ âœ… çŸ¥è¯†æ•´åˆå®Œæˆ (è¾“å‡º{final_length}å­—)")
        gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

    except Exception as e:
        gui_log(f"   â””â”€ âŒ çŸ¥è¯†æ£€ç´¢å¼‚å¸¸: {str(e)[:100]}")
        gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
        logging.error(f"çŸ¥è¯†å¤„ç†æµç¨‹å¼‚å¸¸ï¼š{str(e)}")
        filtered_context = "ï¼ˆçŸ¥è¯†åº“å¤„ç†å¤±è´¥ï¼‰"

    # ä» PromptManager åŠ¨æ€åŠ è½½æç¤ºè¯ï¼ˆå¸¦å¼‚å¸¸ä¿æŠ¤ï¼‰
    try:
        pm = PromptManager()
    except Exception as e:
        logging.error(f"Failed to initialize PromptManager in build_next_chapter_prompt (final): {e}")
        pm = None

    if pm:
        next_prompt_template = pm.get_prompt("chapter", "next_chapter")
    else:
        next_prompt_template = None

    if not next_prompt_template:
        logging.warning("Next chapter prompt not found, using default")
        next_prompt_template = next_chapter_draft_prompt

    # æç¤ºè¯æ„å»ºå®Œæˆï¼š35%
    update_progress("âœ… æç¤ºè¯æ„å»ºå®Œæˆ", 0.35)

    # è¿”å›æœ€ç»ˆæç¤ºè¯
    return next_prompt_template.format(
        user_guidance=user_guidance if user_guidance else "æ— ç‰¹æ®ŠæŒ‡å¯¼",
        global_summary=global_summary_text,
        volume_info=volume_info_text,  # æ–°å¢ï¼šåˆ†å·ä¿¡æ¯
        volume_architecture=current_volume_architecture,  # æ–°å¢ï¼šå·æ¶æ„
        previous_chapter_excerpt=previous_excerpt,
        character_state=character_state_text,
        short_summary=short_summary,
        novel_number=novel_number,
        chapter_title=chapter_title,
        chapter_role=chapter_role,
        chapter_purpose=chapter_purpose,
        suspense_level=suspense_level,
        foreshadowing=foreshadowing,
        plot_twist_level=plot_twist_level,
        volume_position=volume_position,  # ğŸ†• æ–°å¢ï¼šä¼ é€’å·å†…ä½ç½®
        chapter_summary=chapter_summary,
        word_number=word_number,
        characters_involved=characters_involved,
        key_items=key_items,
        scene_location=scene_location,
        time_constraint=time_constraint,
        current_volume_display=current_volume_display,  # æ–°å¢ï¼šå½“å‰å·å±•ç¤º
        next_chapter_number=next_chapter_number,
        next_chapter_title=next_chapter_title,
        next_chapter_role=next_chapter_role,
        next_chapter_purpose=next_chapter_purpose,
        next_chapter_suspense_level=next_chapter_suspense,
        next_chapter_foreshadowing=next_chapter_foreshadow,
        next_chapter_plot_twist_level=next_chapter_twist,
        next_chapter_summary=next_chapter_summary,
        next_volume_display=next_volume_display,  # æ–°å¢ï¼šä¸‹ä¸€ç« å·å±•ç¤º
        filtered_context=filtered_context,
        unresolved_plot_arcs=unresolved_plot_arcs  # ğŸ†• æ³¨å…¥ä¼ç¬”
    )

def generate_chapter_draft(
    api_key: str,
    base_url: str,
    model_name: str,
    filepath: str,
    novel_number: int,
    word_number: int,
    temperature: float,
    user_guidance: str,
    characters_involved: str,
    key_items: str,
    scene_location: str,
    time_constraint: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    embedding_retrieval_k: int = 2,
    interface_format: str = "openai",
    max_tokens: int = 2048,
    timeout: int = 600,
    custom_prompt_text: str = None,
    use_global_system_prompt: bool = False,
    num_volumes: int = 0,  # æ–°å¢ï¼šåˆ†å·æ•°é‡
    total_chapters: int = 0,  # æ–°å¢ï¼šæ€»ç« èŠ‚æ•°
    gui_log_callback=None
) -> str:
    """
    ç”Ÿæˆç« èŠ‚è‰ç¨¿ï¼Œæ”¯æŒè‡ªå®šä¹‰æç¤ºè¯
    """
    # GUIæ—¥å¿—è¾…åŠ©å‡½æ•°
    def gui_log(msg):
        if gui_log_callback:
            gui_log_callback(msg)
        logging.info(msg)

    gui_log(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    gui_log(f"ğŸ“ å¼€å§‹ç”Ÿæˆç¬¬{novel_number}ç« è‰ç¨¿")
    gui_log(f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")

    system_prompt = resolve_global_system_prompt(use_global_system_prompt if use_global_system_prompt is not None else None)

    if custom_prompt_text is None:
        prompt_text = build_chapter_prompt(
            api_key=api_key,
            base_url=base_url,
            model_name=model_name,
            filepath=filepath,
            novel_number=novel_number,
            word_number=word_number,
            temperature=temperature,
            user_guidance=user_guidance,
            characters_involved=characters_involved,
            key_items=key_items,
            scene_location=scene_location,
            time_constraint=time_constraint,
            embedding_api_key=embedding_api_key,
            embedding_url=embedding_url,
            embedding_interface_format=embedding_interface_format,
            embedding_model_name=embedding_model_name,
            embedding_retrieval_k=embedding_retrieval_k,
            interface_format=interface_format,
            max_tokens=max_tokens,
            timeout=timeout,
            system_prompt=system_prompt,
            num_volumes=num_volumes,  # æ–°å¢ï¼šä¼ é€’åˆ†å·å‚æ•°
            total_chapters=total_chapters,  # æ–°å¢ï¼šä¼ é€’æ€»ç« èŠ‚æ•°
            gui_log_callback=gui_log_callback  # ä¼ é€’å›è°ƒ
        )
    else:
        prompt_text = custom_prompt_text

    chapters_dir = os.path.join(filepath, "chapters")
    os.makedirs(chapters_dir, exist_ok=True)

    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )

    gui_log("   â”œâ”€ å‘LLMå‘èµ·è¯·æ±‚ç”Ÿæˆè‰ç¨¿...")
    chapter_content = invoke_with_cleaning(llm_adapter, prompt_text, system_prompt=system_prompt)
    if not chapter_content.strip():
        gui_log("   â””â”€ âš ï¸ ç”Ÿæˆå†…å®¹ä¸ºç©º")
        logging.warning("Generated chapter draft is empty.")
    else:
        gui_log(f"   â””â”€ âœ… è‰ç¨¿ç”Ÿæˆå®Œæˆ (å…±{len(chapter_content)}å­—)\n")

    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    clear_file_content(chapter_file)
    save_string_to_txt(chapter_content, chapter_file)

    gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logging.info(f"[Draft] Chapter {novel_number} generated as a draft.")

    # [Plan C] æ‰¹è¯„å®¶-ä½œå®¶ åŒå¾ªç¯ä¼˜åŒ–
    # ä¼˜å…ˆä½¿ç”¨ PromptManager é…ç½®ï¼›å¦‚æœé…ç½®åŠ è½½å¤±è´¥åˆ™é»˜è®¤ç¦ç”¨ï¼ˆé¿å…æ„å¤–å¢åŠ æˆæœ¬ï¼‰
    pm = None
    try:
        pm = PromptManager()
        enable_refine = pm.is_module_enabled("chapter", "refine")
        enable_critique = pm.is_module_enabled("chapter", "critique")
    except Exception as e:
        logging.warning(f"PromptManager init failed in refine loop, fallback to disabled: {e}")
        enable_refine = False
        enable_critique = False

    # å®Œæ•´çš„æ¡ä»¶åˆ†æ”¯å¤„ç†
    if enable_critique and enable_refine:
        gui_log("\nğŸ“š [Plan C] å¯åŠ¨ã€Œæ‰¹è¯„å®¶-ä½œå®¶ã€ä¼˜åŒ–å¾ªç¯...")

        try:
            directory_file = os.path.join(filepath, "Novel_directory.txt")
            blueprint_text = read_file(directory_file) if os.path.exists(directory_file) else ""
            chap_info = get_chapter_info_from_blueprint(blueprint_text, novel_number) if blueprint_text else {}
            chap_title = chap_info.get("chapter_title", "")

            # è¯»å– Plan C ä¸Šä¸‹æ–‡ï¼ˆå‰æ–‡æè¦ + ä¸Šä¸€ç« ç»“å°¾ï¼‰
            context_short_summary = ""
            context_prev_excerpt = ""

            if novel_number > 1:
                context_cache_file = os.path.join(chapters_dir, f"chapter_{novel_number}_context.json")
                cache_is_fresh = False
                if os.path.exists(context_cache_file):
                    try:
                        cache_mtime = os.path.getmtime(context_cache_file)
                        prev_chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number - 1}.txt")
                        global_summary_file = os.path.join(filepath, "global_summary.txt")

                        cache_is_fresh = True
                        if os.path.exists(prev_chapter_file):
                            if os.path.getmtime(prev_chapter_file) > cache_mtime:
                                cache_is_fresh = False
                        if os.path.exists(global_summary_file):
                            if os.path.getmtime(global_summary_file) > cache_mtime:
                                cache_is_fresh = False

                        if cache_is_fresh:
                            with open(context_cache_file, "r", encoding="utf-8") as f:
                                context_cache = json.load(f)
                            context_short_summary = context_cache.get("short_summary", "").strip()
                            context_prev_excerpt = context_cache.get("previous_chapter_excerpt", "").strip()
                        else:
                            logging.info("Plan C context cache is stale, fallback to latest files")
                    except Exception as e:
                        logging.warning(f"Failed to read Plan C context cache: {e}")

                # è‹¥ç¼“å­˜ç¼ºå¤±ï¼Œå°è¯•ä»ç« èŠ‚æ–‡ä»¶å›é€€
                if not context_prev_excerpt or not cache_is_fresh:
                    recent_texts = get_last_n_chapters_text(chapters_dir, novel_number, n=2)
                    for text in reversed(recent_texts):
                        if text.strip():
                            context_prev_excerpt = text[-800:] if len(text) > 800 else text
                            break

                # è‹¥æ‘˜è¦ç¼ºå¤±ï¼Œå°è¯•ä½¿ç”¨å…¨å±€æ‘˜è¦å…œåº•
                if not context_short_summary or not cache_is_fresh:
                    global_summary_file = os.path.join(filepath, "global_summary.txt")
                    context_short_summary = read_file(global_summary_file).strip()

            if not context_short_summary:
                context_short_summary = "ï¼ˆæš‚æ— å‰æ–‡æ‘˜è¦ï¼‰"
            if not context_prev_excerpt:
                context_prev_excerpt = "ï¼ˆæš‚æ— ä¸Šä¸€ç« å†…å®¹ï¼‰"

            gui_log("   â”œâ”€ æ‰¹è¯„å®¶æ­£åœ¨å®¡é˜…åˆç¨¿...")
            critique_template = pm.get_prompt("chapter", "critique") if pm else ""
            if not critique_template:
                critique_template = chapter_critique_prompt

            try:
                critique_prompt_text = critique_template.format(
                    novel_number=novel_number,
                    chapter_title=chap_title,
                    chapter_text=chapter_content,
                    short_summary=context_short_summary,
                    previous_chapter_excerpt=context_prev_excerpt
                )
            except KeyError as e:
                logging.warning(f"Critique prompt missing placeholder {e}, falling back without context")
                critique_prompt_text = critique_template.format(
                    novel_number=novel_number,
                    chapter_title=chap_title,
                    chapter_text=chapter_content
                )

            critique_response = invoke_with_cleaning(llm_adapter, critique_prompt_text, system_prompt=system_prompt)
            gui_log(f"   â”œâ”€ æ”¶åˆ°ä¿®æ”¹æ„è§ ({len(critique_response)}å­—)")
            logging.info(f"Critique received: {critique_response[:100]}...")

            gui_log("   â”œâ”€ ä½œå®¶æ­£åœ¨æ ¹æ®æ„è§é‡å†™...")
            refine_template = pm.get_prompt("chapter", "refine") if pm else ""
            if not refine_template:
                refine_template = chapter_refine_prompt

            try:
                refine_prompt_text = refine_template.format(
                    critique=critique_response,
                    draft_text=chapter_content,
                    word_number=word_number,
                    short_summary=context_short_summary,
                    previous_chapter_excerpt=context_prev_excerpt
                )
            except KeyError as e:
                logging.warning(f"Refine prompt missing placeholder {e}, falling back without context")
                refine_prompt_text = refine_template.format(
                    critique=critique_response,
                    draft_text=chapter_content,
                    word_number=word_number
                )

            refined_content = invoke_with_cleaning(llm_adapter, refine_prompt_text, system_prompt=system_prompt)

            # å¢å¼ºæœ‰æ•ˆæ€§æ ¡éªŒ
            is_valid = _is_valid_refined_content(refined_content, len(chapter_content))

            if is_valid:
                chapter_content = refined_content

                clear_file_content(chapter_file)
                save_string_to_txt(chapter_content, chapter_file)

                # ä¿å­˜ä¼˜åŒ–è®°å½•ï¼ˆå¯é€‰ï¼Œæ–¹ä¾¿è°ƒè¯•ï¼‰
                debug_file = os.path.join(chapters_dir, f"chapter_{novel_number}_critique.txt")
                save_string_to_txt(
                    f"Critique:\n{critique_response}\n\nRefined:\n{refined_content}",
                    debug_file
                )

                gui_log("   â””â”€ âœ… ä¼˜åŒ–å®Œæˆï¼Œå·²æ›¿æ¢åˆç¨¿")
            else:
                gui_log("   â””â”€ âš ï¸ é‡å†™å†…å®¹æ— æ•ˆï¼Œä¿ç•™åˆç¨¿")
                logging.warning(f"Refined content invalid: length={len(refined_content) if refined_content else 0}")

        except Exception as e:
            gui_log(f"   â””â”€ âŒ ä¼˜åŒ–å¾ªç¯å¼‚å¸¸: {str(e)}")
            logging.error(f"Refine loop error: {e}")

    elif enable_critique and not enable_refine:
        gui_log("â–· [Plan C] ä»…å¯ç”¨æ‰¹è¯„å®¶ï¼Œè·³è¿‡é‡å†™ï¼ˆéœ€åŒæ—¶å¯ç”¨ refine æ¨¡å—ï¼‰\n")
    elif not enable_critique and enable_refine:
        gui_log("â–· [Plan C] ä½œå®¶æ¨¡å—ä¾èµ–æ‰¹è¯„å®¶ï¼Œè·³è¿‡é‡å†™ï¼ˆéœ€åŒæ—¶å¯ç”¨ critique æ¨¡å—ï¼‰\n")
    else:
        # ä¸¤ä¸ªéƒ½ç¦ç”¨ï¼Œé™é»˜è·³è¿‡ï¼ˆä¸è¾“å‡ºæ—¥å¿—ï¼Œé¿å…å¹²æ‰°ï¼‰
        logging.debug("[Plan C] Both critique and refine disabled, skipping optimization loop")

    return chapter_content


def _is_valid_refined_content(content: str, draft_length: int) -> bool:
    """
    [Plan C] æ ¡éªŒé‡å†™å†…å®¹æ˜¯å¦æœ‰æ•ˆ

    æ£€æŸ¥è§„åˆ™ï¼š
    1. å†…å®¹ä¸èƒ½ä¸ºç©ºæˆ–è¿‡çŸ­ï¼ˆ<500å­—ï¼‰
    2. ä¸èƒ½æ˜¯ LLM çš„æ‹’ç»å“åº”
    3. ä¸èƒ½æ¯”åˆç¨¿ç¼©æ°´å¤ªå¤šï¼ˆ<40%ï¼‰
    """
    if not content:
        return False

    content_length = len(content)

    # é•¿åº¦è¿‡çŸ­
    if content_length < 500:
        logging.debug(f"Refined content too short: {content_length}")
        return False

    # æ£€æŸ¥æ˜¯å¦æ˜¯ LLM æ‹’ç»å“åº”çš„å¸¸è§æ¨¡å¼
    refusal_patterns = [
        "æŠ±æ­‰", "æ— æ³•", "ä¸èƒ½", "å¯¹ä¸èµ·", "å¾ˆé—æ†¾",
        "I cannot", "I'm sorry", "I apologize", "unable to"
    ]
    first_100_chars = content[:100]
    for pattern in refusal_patterns:
        if pattern in first_100_chars:
            logging.debug(f"Refined content appears to be a refusal: found '{pattern}'")
            return False

    # æ¯”åˆç¨¿ç¼©æ°´å¤ªå¤šï¼ˆä½äº40%ï¼‰
    if draft_length > 0 and content_length < draft_length * 0.4:
        logging.debug(f"Refined content too short compared to draft: {content_length} vs {draft_length}")
        return False

    return True









