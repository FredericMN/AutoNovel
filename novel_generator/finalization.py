#novel_generator/finalization.py
# -*- coding: utf-8 -*-
"""
å®šç¨¿ç« èŠ‚å’Œæ‰©å†™ç« èŠ‚ï¼ˆfinalize_chapterã€enrich_chapter_textï¼‰
"""
import os
import logging
from llm_adapters import create_llm_adapter
from embedding_adapters import create_embedding_adapter
from prompt_definitions import (
    summary_prompt,
    update_character_state_prompt,
    volume_summary_prompt,  # æ–°å¢ï¼šåˆ†å·æ€»ç»“æç¤ºè¯
    resolve_global_system_prompt
)
from novel_generator.common import invoke_with_cleaning
from utils import read_file, clear_file_content, save_string_to_txt
from novel_generator.vectorstore_utils import update_vector_store
from volume_utils import calculate_volume_ranges, is_volume_last_chapter  # æ–°å¢ï¼šåˆ†å·å·¥å…·å‡½æ•°
logging.basicConfig(
    filename='app.log',      # æ—¥å¿—æ–‡ä»¶å
    filemode='a',            # è¿½åŠ æ¨¡å¼ï¼ˆ'w' ä¼šè¦†ç›–ï¼‰
    level=logging.INFO,      # è®°å½• INFO åŠä»¥ä¸Šçº§åˆ«çš„æ—¥å¿—
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
def finalize_volume(
    volume_number: int,
    volume_start: int,
    volume_end: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    filepath: str,
    interface_format: str,
    max_tokens: int,
    timeout: int = 600,
    use_global_system_prompt: bool = False,
    gui_log_callback=None
):
    """
    ä¸ºæŒ‡å®šå·ç”Ÿæˆæ€»ç»“æ‘˜è¦ï¼ˆä»…åˆ†å·æ¨¡å¼ï¼‰

    Args:
        volume_number: å·å·ï¼ˆ1-basedï¼‰
        volume_start: å·çš„èµ·å§‹ç« èŠ‚å·
        volume_end: å·çš„ç»“æŸç« èŠ‚å·
        å…¶ä»–å‚æ•°åŒ finalize_chapter

    ç”Ÿæˆæ–‡ä»¶ï¼š
        - volume_X_summary.txt: å·æ‘˜è¦
        - æ¸…ç©º global_summary.txt ä¸ºä¸‹ä¸€å·åšå‡†å¤‡
    """
    def gui_log(msg):
        if gui_log_callback:
            gui_log_callback(msg)
        logging.info(msg)

    gui_log(f"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    gui_log(f"ğŸ“– å¼€å§‹ç”Ÿæˆç¬¬{volume_number}å·æ€»ç»“")
    gui_log(f"   å·èŒƒå›´: ç¬¬{volume_start}-{volume_end}ç« ")
    gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

    # è¯»å–è¯¥å·çš„æ‰€æœ‰ç« èŠ‚å†…å®¹
    chapters_dir = os.path.join(filepath, "chapters")
    volume_chapters_text = []

    gui_log(f"â–¶ è¯»å–ç¬¬{volume_start}-{volume_end}ç« å†…å®¹...")
    for chap_num in range(volume_start, volume_end + 1):
        chapter_file = os.path.join(chapters_dir, f"chapter_{chap_num}.txt")
        if os.path.exists(chapter_file):
            chapter_text = read_file(chapter_file).strip()
            if chapter_text:
                volume_chapters_text.append(f"=== ç¬¬{chap_num}ç«  ===\n{chapter_text}")
        else:
            gui_log(f"âš ï¸ ç¬¬{chap_num}ç« æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡")

    if not volume_chapters_text:
        gui_log("âŒ è¯¥å·æ²¡æœ‰å¯ç”¨çš„ç« èŠ‚å†…å®¹ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“")
        logging.warning(f"Volume {volume_number} has no chapter content.")
        return

    combined_volume_text = "\n\n".join(volume_chapters_text)

    # é™åˆ¶æ€»æ–‡æœ¬é•¿åº¦ï¼ˆé¿å…è¶…è¿‡ context çª—å£ï¼‰
    max_combined_length = 50000  # çº¦50Kå­—ç¬¦
    if len(combined_volume_text) > max_combined_length:
        gui_log(f"âš ï¸ å·å†…å®¹è¿‡é•¿({len(combined_volume_text)}å­—)ï¼Œæˆªå–å{max_combined_length}å­—ç¬¦")
        combined_volume_text = combined_volume_text[-max_combined_length:]

    # è¯»å–å·æ¶æ„ä¿¡æ¯
    volume_arch_file = os.path.join(filepath, "Volume_architecture.txt")
    volume_architecture_text = ""
    if os.path.exists(volume_arch_file):
        volume_architecture_text = read_file(volume_arch_file).strip()

    # æ„å»º LLM é€‚é…å™¨
    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    system_prompt = resolve_global_system_prompt(use_global_system_prompt)

    # ç”Ÿæˆå·æ‘˜è¦
    gui_log("â–¶ å‘LLMå‘èµ·è¯·æ±‚ç”Ÿæˆå·æ‘˜è¦...")
    volume_summary_prompt_text = volume_summary_prompt.format(
        volume_number=volume_number,
        volume_start=volume_start,
        volume_end=volume_end,
        volume_chapters_text=combined_volume_text,
        volume_architecture=volume_architecture_text
    )

    volume_summary_result = invoke_with_cleaning(
        llm_adapter,
        volume_summary_prompt_text,
        system_prompt=system_prompt
    )

    if not volume_summary_result.strip():
        gui_log("   â””â”€ âŒ ç”Ÿæˆå¤±è´¥")
        logging.warning(f"Volume {volume_number} summary generation failed.")
        return

    gui_log(f"   â””â”€ âœ… å·æ‘˜è¦ç”Ÿæˆå®Œæˆ (å…±{len(volume_summary_result)}å­—)\n")

    # ä¿å­˜å·æ‘˜è¦
    volume_summary_file = os.path.join(filepath, f"volume_{volume_number}_summary.txt")
    clear_file_content(volume_summary_file)
    save_string_to_txt(volume_summary_result, volume_summary_file)
    gui_log(f"â–¶ å·æ‘˜è¦å·²ä¿å­˜è‡³: volume_{volume_number}_summary.txt")

    # å°†å·æ‘˜è¦ä¹Ÿå­˜å…¥å‘é‡åº“ï¼ˆæ ‡è®°ä¸ºç‰¹æ®Šç±»å‹ï¼Œæ–¹ä¾¿è·¨å·æ£€ç´¢ï¼‰
    try:
        from embedding_adapters import create_embedding_adapter
        # å°è¯•ä»é…ç½®è¯»å– embedding é…ç½®ï¼ˆé™çº§ç­–ç•¥ï¼šå¦‚æœæ— æ³•è·å–åˆ™è·³è¿‡ï¼‰
        import json
        config_file = "config.json"
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                embedding_config = config.get("embedding_configs", {})
                if embedding_config:
                    embedding_adapter = create_embedding_adapter(
                        embedding_config.get("interface_format", "openai"),
                        embedding_config.get("api_key", ""),
                        embedding_config.get("base_url", ""),
                        embedding_config.get("model_name", "text-embedding-ada-002")
                    )

                    # å…ˆåˆ é™¤æ—§çš„å·æ‘˜è¦ï¼ˆé¿å…é‡å¤å­˜å‚¨ï¼‰
                    from novel_generator.vectorstore_utils import delete_volume_summary_from_store
                    delete_volume_summary_from_store(embedding_adapter, filepath, volume_number)
                    gui_log(f"â–¶ å·²æ¸…ç†æ—§å·æ‘˜è¦ï¼ˆå¦‚å­˜åœ¨ï¼‰")

                    # å°†å·æ‘˜è¦åˆ‡åˆ†åå­˜å…¥å‘é‡åº“ï¼Œæ ‡è®°ä¸ºå·æ‘˜è¦ç±»å‹
                    from novel_generator.vectorstore_utils import update_vector_store

                    # æ„å»ºå·æ‘˜è¦æ ‡é¢˜ï¼ˆä¾¿äºæ£€ç´¢æ—¶è¯†åˆ«ï¼‰
                    volume_summary_with_title = f"ã€ç¬¬{volume_number}å·æ€»ç»“ã€‘\n{volume_summary_result}"

                    update_vector_store(
                        embedding_adapter=embedding_adapter,
                        new_chapter=volume_summary_with_title,
                        filepath=filepath,
                        chapter_num=volume_end,  # ä½¿ç”¨å·çš„æœ«ç« å·ä½œä¸ºæ ‡è®°
                        volume_num=volume_number
                    )
                    gui_log(f"â–¶ å·æ‘˜è¦å·²å­˜å…¥å‘é‡åº“ï¼ˆä¾¿äºè·¨å·æ£€ç´¢ï¼‰\n")
                    logging.info(f"Volume {volume_number} summary stored in vector store")
    except Exception as e:
        # éå…³é”®æ“ä½œï¼Œå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        logging.warning(f"Failed to store volume summary in vector store: {e}")
        gui_log(f"âš ï¸ å·æ‘˜è¦å‘é‡å­˜å‚¨å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {str(e)[:50]}\n")

    # æ¸…ç©ºå…¨å±€æ‘˜è¦ï¼Œä¸ºä¸‹ä¸€å·åšå‡†å¤‡
    global_summary_file = os.path.join(filepath, "global_summary.txt")
    clear_file_content(global_summary_file)
    gui_log("â–¶ å·²æ¸…ç©º global_summary.txtï¼Œä¸ºä¸‹ä¸€å·åšå‡†å¤‡\n")

    gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    gui_log(f"âœ… ç¬¬{volume_number}å·æ€»ç»“å®Œæˆ")
    gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logging.info(f"Volume {volume_number} summary has been generated successfully.")


def finalize_chapter(
    novel_number: int,
    word_number: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    filepath: str,
    embedding_api_key: str,
    embedding_url: str,
    embedding_interface_format: str,
    embedding_model_name: str,
    interface_format: str,
    max_tokens: int,
    timeout: int = 600,
    use_global_system_prompt: bool = False,
    num_volumes: int = 0,  # æ–°å¢ï¼šåˆ†å·æ•°é‡
    total_chapters: int = 0,  # æ–°å¢ï¼šæ€»ç« èŠ‚æ•°
    gui_log_callback=None
):
    """
    å¯¹æŒ‡å®šç« èŠ‚åšæœ€ç»ˆå¤„ç†ï¼šæ›´æ–°å‰æ–‡æ‘˜è¦ã€æ›´æ–°è§’è‰²çŠ¶æ€ã€æ’å…¥å‘é‡åº“ç­‰ã€‚
    é»˜è®¤æ— éœ€å†åšæ‰©å†™æ“ä½œï¼Œè‹¥æœ‰éœ€è¦å¯åœ¨å¤–éƒ¨è°ƒç”¨ enrich_chapter_text å¤„ç†åå†å®šç¨¿ã€‚
    """
    # GUIæ—¥å¿—è¾…åŠ©å‡½æ•°
    def gui_log(msg):
        if gui_log_callback:
            gui_log_callback(msg)
        logging.info(msg)

    gui_log("\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    gui_log(f"ğŸ“ å¼€å§‹å®šç¨¿ç¬¬{novel_number}ç« ")
    gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

    chapters_dir = os.path.join(filepath, "chapters")
    chapter_file = os.path.join(chapters_dir, f"chapter_{novel_number}.txt")
    chapter_text = read_file(chapter_file).strip()
    if not chapter_text:
        gui_log("âŒ ç« èŠ‚æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•å®šç¨¿")
        logging.warning(f"Chapter {novel_number} is empty, cannot finalize.")
        return

    gui_log(f"â–¶ [1/3] æ›´æ–°å‰æ–‡æ‘˜è¦")
    gui_log("   â”œâ”€ è¯»å–æ—§æ‘˜è¦...")
    global_summary_file = os.path.join(filepath, "global_summary.txt")
    old_global_summary = read_file(global_summary_file)
    character_state_file = os.path.join(filepath, "character_state.txt")
    old_character_state = read_file(character_state_file)

    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    system_prompt = resolve_global_system_prompt(use_global_system_prompt)

    prompt_summary = summary_prompt.format(
        chapter_text=chapter_text,
        global_summary=old_global_summary
    )
    gui_log("   â”œâ”€ å‘LLMå‘èµ·è¯·æ±‚...")
    new_global_summary = invoke_with_cleaning(llm_adapter, prompt_summary, system_prompt=system_prompt)
    if not new_global_summary.strip():
        gui_log("   â”œâ”€ âš  ç”Ÿæˆå¤±è´¥ï¼Œä¿ç•™æ—§æ‘˜è¦")
        new_global_summary = old_global_summary
    else:
        gui_log("   â””â”€ âœ… å‰æ–‡æ‘˜è¦æ›´æ–°å®Œæˆ\n")

    gui_log("â–¶ [2/3] æ›´æ–°è§’è‰²çŠ¶æ€")
    gui_log("   â”œâ”€ è¯»å–æ—§çŠ¶æ€...")
    prompt_char_state = update_character_state_prompt.format(
        chapter_text=chapter_text,
        old_state=old_character_state
    )
    gui_log("   â”œâ”€ å‘LLMå‘èµ·è¯·æ±‚...")
    new_char_state = invoke_with_cleaning(llm_adapter, prompt_char_state, system_prompt=system_prompt)
    if not new_char_state.strip():
        gui_log("   â”œâ”€ âš  ç”Ÿæˆå¤±è´¥ï¼Œä¿ç•™æ—§çŠ¶æ€")
        new_char_state = old_character_state
    else:
        gui_log("   â””â”€ âœ… è§’è‰²çŠ¶æ€æ›´æ–°å®Œæˆ\n")

    gui_log("   â”œâ”€ ä¿å­˜æ›´æ–°ç»“æœ...")
    clear_file_content(global_summary_file)
    save_string_to_txt(new_global_summary, global_summary_file)
    clear_file_content(character_state_file)
    save_string_to_txt(new_char_state, character_state_file)

    gui_log("â–¶ [3/3] æ’å…¥å‘é‡åº“")
    gui_log("   â”œâ”€ åˆ‡åˆ†ç« èŠ‚æ–‡æœ¬...")

    # è®¡ç®—å·å·ï¼ˆç”¨äºå‘é‡æ£€ç´¢ä¼˜åŒ–ï¼‰
    volume_num = None
    if num_volumes > 1 and total_chapters > 0:
        from volume_utils import get_volume_number, calculate_volume_ranges
        volume_ranges = calculate_volume_ranges(total_chapters, num_volumes)
        volume_num = get_volume_number(novel_number, volume_ranges)
        gui_log(f"   â”œâ”€ ç« èŠ‚å…ƒæ•°æ®: chapter={novel_number}, volume={volume_num}")

    update_vector_store(
        embedding_adapter=create_embedding_adapter(
            embedding_interface_format,
            embedding_api_key,
            embedding_url,
            embedding_model_name
        ),
        new_chapter=chapter_text,
        filepath=filepath,
        chapter_num=novel_number,  # æ–°å¢ï¼šç« èŠ‚å·
        volume_num=volume_num  # æ–°å¢ï¼šå·å·
    )
    gui_log("   â””â”€ âœ… å‘é‡åº“æ›´æ–°å®Œæˆ\n")

    gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    gui_log(f"âœ… ç¬¬{novel_number}ç« å®šç¨¿å®Œæˆ")
    gui_log("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    logging.info(f"Chapter {novel_number} has been finalized.")

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆå·æ€»ç»“ï¼ˆåˆ†å·æ¨¡å¼ + å·æœ«ç« èŠ‚ï¼‰
    if num_volumes > 1 and total_chapters > 0:
        volume_ranges = calculate_volume_ranges(total_chapters, num_volumes)

        if is_volume_last_chapter(novel_number, volume_ranges):
            from volume_utils import get_volume_number

            volume_num = get_volume_number(novel_number, volume_ranges)
            if volume_num > 0:
                vol_start, vol_end = volume_ranges[volume_num - 1]

                gui_log(f"\nğŸ”” æ£€æµ‹åˆ°ç¬¬{novel_number}ç« æ˜¯ç¬¬{volume_num}å·çš„æœ€åä¸€ç« ")
                gui_log("   å¯åŠ¨å·æ€»ç»“ç”Ÿæˆæµç¨‹...\n")

                finalize_volume(
                    volume_number=volume_num,
                    volume_start=vol_start,
                    volume_end=vol_end,
                    api_key=api_key,
                    base_url=base_url,
                    model_name=model_name,
                    temperature=temperature,
                    filepath=filepath,
                    interface_format=interface_format,
                    max_tokens=max_tokens,
                    timeout=timeout,
                    use_global_system_prompt=use_global_system_prompt,
                    gui_log_callback=gui_log_callback
                )

def enrich_chapter_text(
    chapter_text: str,
    word_number: int,
    api_key: str,
    base_url: str,
    model_name: str,
    temperature: float,
    interface_format: str,
    max_tokens: int,
    timeout: int = 600,
    use_global_system_prompt: bool = False
) -> str:
    """
    å¯¹ç« èŠ‚æ–‡æœ¬è¿›è¡Œæ‰©å†™ï¼Œä½¿å…¶æ›´æ¥è¿‘ word_number å­—æ•°ï¼Œä¿æŒå‰§æƒ…è¿è´¯ã€‚
    """
    llm_adapter = create_llm_adapter(
        interface_format=interface_format,
        base_url=base_url,
        model_name=model_name,
        api_key=api_key,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout
    )
    system_prompt = resolve_global_system_prompt(use_global_system_prompt)
    prompt = f"""ä»¥ä¸‹ç« èŠ‚æ–‡æœ¬è¾ƒçŸ­ï¼Œè¯·åœ¨ä¿æŒå‰§æƒ…è¿è´¯çš„å‰æä¸‹è¿›è¡Œæ‰©å†™ï¼Œä½¿å…¶æ›´å……å®ï¼Œæ¥è¿‘ {word_number} å­—å·¦å³ï¼Œä»…ç»™å‡ºæœ€ç»ˆæ–‡æœ¬ï¼Œä¸è¦è§£é‡Šä»»ä½•å†…å®¹ã€‚ï¼š
åŸå†…å®¹ï¼š
{chapter_text}
"""
    enriched_text = invoke_with_cleaning(llm_adapter, prompt, system_prompt=system_prompt)
    return enriched_text if enriched_text else chapter_text
